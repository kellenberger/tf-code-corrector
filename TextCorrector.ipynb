{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sven/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import nltk\n",
    "\n",
    "from utils.standard_hparams_utils import standard_hparams\n",
    "from train import train\n",
    "from nmt import create_hparams, create_or_load_hparams\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_path = \"/home/sven/dialog_corpus\"\n",
    "# root_data_path = \"E:\\\\Documents\\\\dialog_corpus\"\n",
    "full_path = os.path.join(root_data_path, \"movie_lines.txt\")\n",
    "train_path = os.path.join(root_data_path, \"train.tgt\")\n",
    "perturbed_train_path = os.path.join(root_data_path, \"train.src\")\n",
    "val_path = os.path.join(root_data_path, \"val.tgt\")\n",
    "perturbed_val_path = os.path.join(root_data_path, \"val.src\")\n",
    "test_path = os.path.join(root_data_path, \"test.tgt\")\n",
    "perturbed_test_path = os.path.join(root_data_path, \"test.src\")\n",
    "tgt_vocab_path = os.path.join(root_data_path, \"vocab.tgt\")\n",
    "src_vocab_path = os.path.join(root_data_path, \"vocab.src\")\n",
    "model_path = os.path.join(root_data_path, \"dialog_correcter_model_testnltk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(root_data_path, \"movie_lines_raw.txt\"), \"r\") as raw_data, \\\n",
    "        io.open(full_path, \"w\", encoding='utf-8') as out:\n",
    "    for line in raw_data:\n",
    "        parts = line.split(\" +++$+++ \")\n",
    "        dialog_line = parts[-1]\n",
    "        s = dialog_line.strip().lower()\n",
    "        preprocessed_line = \" \".join(nltk.word_tokenize(s))\n",
    "        if preprocessed_line:\n",
    "            out.write(unicode(\"\\n\" + preprocessed_line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Unparser' object has no attribute '_str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-66babc506a1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mast_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mast_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mast_tree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mast_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mastunparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mast_tree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\astunparse\\__init__.py\u001b[0m in \u001b[0;36munparse\u001b[1;34m(tree)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0munparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcStringIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mUnparser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\astunparse\\unparser.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, tree, file)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfuture_imports\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_indent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\astunparse\\unparser.py\u001b[0m in \u001b[0;36mdispatch\u001b[1;34m(self, tree)\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Unparser' object has no attribute '_str'"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import dis\n",
    "import astunparse\n",
    "ast_path = \"E:\\\\Downloads\\\\py150\\\\python100k_train.json\"\n",
    "with open(ast_path, \"r\") as ast_data:\n",
    "    ast_tree = ast_data.readline()\n",
    "    print(astunparse.unparse((ast.parse(ast_tree).body)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Lines: 303837/304447, 99.7996367184%\n",
      "Val Lines: 304/304447, 0.0998531764149%\n",
      "Test Lines: 305/304447, 0.100181640811%\n"
     ]
    }
   ],
   "source": [
    "DROPOUT_TOKENS = {\"a\", \"an\", \"the\", \"'ll\", \"'s\", \"'m\", \"'ve\"}\n",
    "\n",
    "REPLACEMENTS = {\"there\": \"their\", \"their\": \"there\", \"then\": \"than\", \"than\": \"then\"}\n",
    "\n",
    "count = 0\n",
    "with open(full_path, \"r\") as raw_data, \\\n",
    "        io.open(tgt_vocab_path, \"w\", encoding=\"utf-8\") as tgt_vocab, \\\n",
    "        io.open(src_vocab_path, \"w\", encoding=\"utf-8\") as src_vocab:\n",
    "    word_counter = Counter(raw_data.read().split())\n",
    "    most_frequent_words = [ word for (word, frequency) in word_counter.most_common(50000)]\n",
    "    tgt_vocab.write(unicode(\"\\n\".join(most_frequent_words)))\n",
    "    src_vocab.write(unicode(\"\\n\".join(most_frequent_words)))\n",
    "    \n",
    "with open(full_path, \"r\") as raw_data:\n",
    "    for line in raw_data:\n",
    "        count += 1\n",
    "            \n",
    "train_count = 0\n",
    "val_count = 0\n",
    "test_count = 0\n",
    "with open(full_path, \"r\") as raw_data, \\\n",
    "            io.open(train_path, \"w\", encoding=\"utf-8\") as tgt_train, \\\n",
    "            io.open(perturbed_train_path, \"w\", encoding=\"utf-8\") as src_train, \\\n",
    "            io.open(val_path, \"w\", encoding=\"utf-8\") as tgt_val, \\\n",
    "            io.open(perturbed_val_path, \"w\", encoding=\"utf-8\") as src_val, \\\n",
    "            io.open(test_path, \"w\", encoding=\"utf-8\") as tgt_test, \\\n",
    "            io.open(perturbed_test_path, \"w\", encoding=\"utf-8\") as src_test:\n",
    "        for i, line in enumerate(raw_data):\n",
    "            if not line.strip(): continue\n",
    "            tokens = line.lower().strip().split()\n",
    "            source = []\n",
    "            target = []\n",
    "\n",
    "            for token in tokens:\n",
    "                target.append(token)\n",
    "\n",
    "                # Randomly dropout some words from the input.\n",
    "                dropout_token = (token in DROPOUT_TOKENS and\n",
    "                                random.random() < 0.25)\n",
    "                replace_token = (token in REPLACEMENTS and\n",
    "                                random.random() < 0.25)\n",
    "\n",
    "                if replace_token:\n",
    "                    source.append(REPLACEMENTS[token])\n",
    "                elif not dropout_token:\n",
    "                    source.append(token)\n",
    "                    \n",
    "            if i+1 < 0.998*count:\n",
    "                tgt_train.write(unicode(\" \".join(target) + \"\\n\"))\n",
    "                src_train.write(unicode(\" \".join(source) + \"\\n\"))\n",
    "                train_count += 1\n",
    "            elif i+1 < 0.999*count:\n",
    "                tgt_val.write(unicode(\" \".join(target) + \"\\n\"))\n",
    "                src_val.write(unicode(\" \".join(source) + \"\\n\"))\n",
    "                val_count += 1\n",
    "            else:\n",
    "                tgt_test.write(unicode(\" \".join(target) + \"\\n\"))\n",
    "                src_test.write(unicode(\" \".join(source) + \"\\n\"))\n",
    "                test_count += 1\n",
    "print(\"Train Lines: {}/{}, {}%\".format(train_count, count, train_count * 100.0 / count))\n",
    "print(\"Val Lines: {}/{}, {}%\".format(val_count, count, val_count * 100.0 / count))\n",
    "print(\"Test Lines: {}/{}, {}%\".format(test_count, count, test_count * 100.0 / count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "483441\n"
     ]
    }
   ],
   "source": [
    "with open(full_path, \"r\") as raw_data:\n",
    "    word_counter = Counter(raw_data.read().split())\n",
    "    print(len(list(word_counter.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "with open(train_path, \"r\") as infile:\n",
    "    for line in infile:\n",
    "        if not line.strip(): \n",
    "            count += 1\n",
    "print(count)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# hparams:\n",
      "  src=src\n",
      "  tgt=tgt\n",
      "  train_prefix=/home/sven/dialog_corpus/train\n",
      "  dev_prefix=/home/sven/dialog_corpus/val\n",
      "  test_prefix=/home/sven/dialog_corpus/test\n",
      "  out_dir=/home/sven/dialog_corpus/nmt_attention_model\n",
      "# Vocab file /home/sven/dialog_corpus/vocab.src exists\n",
      "The first 3 vocab words [., ,, you] are not [<unk>, <s>, </s>]\n",
      "# Vocab file /home/sven/dialog_corpus/vocab.tgt exists\n",
      "The first 3 vocab words [., ,, you] are not [<unk>, <s>, </s>]\n",
      "  attention=scaled_luong\n",
      "  attention_architecture=standard\n",
      "  avg_ckpts=False\n",
      "  batch_size=128\n",
      "  beam_width=0\n",
      "  best_bleu=0\n",
      "  best_bleu_dir=/home/sven/dialog_corpus/nmt_attention_model/best_bleu\n",
      "  check_special_token=True\n",
      "  colocate_gradients_with_ops=True\n",
      "  decay_scheme=\n",
      "  dev_prefix=/home/sven/dialog_corpus/val\n",
      "  dropout=0.2\n",
      "  embed_prefix=None\n",
      "  encoder_type=uni\n",
      "  eos=</s>\n",
      "  epoch_step=0\n",
      "  forget_bias=1.0\n",
      "  infer_batch_size=32\n",
      "  init_op=uniform\n",
      "  init_weight=0.1\n",
      "  learning_rate=1.0\n",
      "  length_penalty_weight=0.0\n",
      "  log_device_placement=False\n",
      "  max_gradient_norm=5.0\n",
      "  max_train=0\n",
      "  metrics=['bleu']\n",
      "  num_buckets=5\n",
      "  num_decoder_layers=2\n",
      "  num_decoder_residual_layers=0\n",
      "  num_embeddings_partitions=0\n",
      "  num_encoder_layers=2\n",
      "  num_encoder_residual_layers=0\n",
      "  num_gpus=1\n",
      "  num_inter_threads=0\n",
      "  num_intra_threads=0\n",
      "  num_keep_ckpts=5\n",
      "  num_layers=2\n",
      "  num_train_steps=12000\n",
      "  num_translations_per_input=1\n",
      "  num_units=128\n",
      "  optimizer=sgd\n",
      "  out_dir=/home/sven/dialog_corpus/nmt_attention_model\n",
      "  output_attention=True\n",
      "  override_loaded_hparams=False\n",
      "  pass_hidden_state=True\n",
      "  random_seed=None\n",
      "  residual=False\n",
      "  sampling_temperature=0.0\n",
      "  share_vocab=False\n",
      "  sos=<s>\n",
      "  src=src\n",
      "  src_embed_file=\n",
      "  src_max_len=50\n",
      "  src_max_len_infer=None\n",
      "  src_vocab_file=/home/sven/dialog_corpus/nmt_attention_model/vocab.src\n",
      "  src_vocab_size=50003\n",
      "  steps_per_external_eval=None\n",
      "  steps_per_stats=100\n",
      "  subword_option=\n",
      "  test_prefix=/home/sven/dialog_corpus/test\n",
      "  tgt=tgt\n",
      "  tgt_embed_file=\n",
      "  tgt_max_len=50\n",
      "  tgt_max_len_infer=None\n",
      "  tgt_vocab_file=/home/sven/dialog_corpus/nmt_attention_model/vocab.tgt\n",
      "  tgt_vocab_size=50003\n",
      "  time_major=True\n",
      "  train_prefix=/home/sven/dialog_corpus/train\n",
      "  unit_type=lstm\n",
      "  vocab_prefix=/home/sven/dialog_corpus/vocab\n",
      "  warmup_scheme=t2t\n",
      "  warmup_steps=0\n"
     ]
    }
   ],
   "source": [
    "standard_flags = standard_hparams()\n",
    "flags = {\n",
    "    \"attention\": \"scaled_luong\",\n",
    "    \"src\": \"src\",\n",
    "    \"tgt\": \"tgt\",\n",
    "    \"vocab_prefix\": os.path.join(root_data_path, \"vocab\"),\n",
    "    \"train_prefix\": os.path.join(root_data_path, \"train\"),\n",
    "    \"dev_prefix\": os.path.join(root_data_path, \"val\"),\n",
    "    \"test_prefix\": os.path.join(root_data_path, \"test\"),\n",
    "    \"out_dir\": os.path.join(root_data_path, 'nmt_attention_model'),\n",
    "    \"num_train_steps\": 12000,\n",
    "    \"steps_per_stats\": 100,\n",
    "    \"num_layers\": 2,\n",
    "    \"num_units\": 128,\n",
    "    \"dropout\": 0.2,\n",
    "    \"metrics\": \"bleu\"\n",
    "}\n",
    "standard_flags.update(flags)\n",
    "hparams = create_or_load_hparams(standard_flags[\"out_dir\"], \\\n",
    "                                 create_hparams(objectview(standard_flags)), \\\n",
    "                                 standard_flags[\"hparams_path\"], \\\n",
    "                                 save_hparams=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# creating train graph ...\n",
      "  num_layers = 2, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DropoutWrapper, dropout=0.2   DeviceWrapper, device=/gpu:0\n",
      "  learning_rate=1, warmup_steps=0, warmup_scheme=t2t\n",
      "  decay_scheme=, start_decay_step=12000, decay_steps 0, decay_factor 1\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (50003, 128), /device:CPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (50003, 128), /device:CPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), \n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 50003), \n",
      "# creating eval graph ...\n",
      "  num_layers = 2, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (50003, 128), /device:CPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (50003, 128), /device:CPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), \n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 50003), \n",
      "# creating infer graph ...\n",
      "  num_layers = 2, num_residual_layers=0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 0  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "  cell 1  LSTM, forget_bias=1  DeviceWrapper, device=/gpu:0\n",
      "# Trainable variables\n",
      "  embeddings/encoder/embedding_encoder:0, (50003, 128), /device:CPU:0\n",
      "  embeddings/decoder/embedding_decoder:0, (50003, 128), /device:CPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/memory_layer/kernel:0, (128, 128), \n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0, (384, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0, (256, 512), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0, (512,), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/luong_attention/attention_g:0, (), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/attention/attention_layer/kernel:0, (256, 128), /device:GPU:0\n",
      "  dynamic_seq2seq/decoder/output_projection/kernel:0, (128, 50003), \n",
      "# log_file=/home/sven/dialog_corpus/nmt_attention_model/log_1527885237\n",
      "  created train model with fresh parameters, time 0.36s\n",
      "  created infer model with fresh parameters, time 0.26s\n",
      "  # 226\n",
      "    src: it was n't me .\n",
      "    ref: it was n't me .\n",
      "    nmt: just..trying just..trying klute klute 33.50 diabetic diabetic diabetic diabetic diabetic\n",
      "  created eval model with fresh parameters, time 0.30s\n",
      "  eval dev: perplexity 49988.82, time 4s, Fri Jun  1 22:34:03 2018.\n",
      "  eval test: perplexity 49985.06, time 4s, Fri Jun  1 22:34:07 2018.\n",
      "  created infer model with fresh parameters, time 0.20s\n",
      "# Start step 0, lr 1, Fri Jun  1 22:34:07 2018\n",
      "# Init train iterator, skipping 0 elements\n",
      "  step 100 lr 1 step-time 4.07s wps 0.82K ppl 97012.13 gN 68.27 bleu 0.00, Fri Jun  1 22:40:55 2018\n",
      "  step 200 lr 1 step-time 3.99s wps 0.85K ppl 1781.80 gN 16.56 bleu 0.00, Fri Jun  1 22:47:34 2018\n",
      "  step 300 lr 1 step-time 4.10s wps 0.87K ppl 860.38 gN 9.57 bleu 0.00, Fri Jun  1 22:54:23 2018\n",
      "  step 400 lr 1 step-time 3.96s wps 0.85K ppl 455.86 gN 5.99 bleu 0.00, Fri Jun  1 23:00:59 2018\n",
      "  step 500 lr 1 step-time 3.96s wps 0.86K ppl 307.47 gN 4.94 bleu 0.00, Fri Jun  1 23:07:35 2018\n",
      "  step 600 lr 1 step-time 4.09s wps 0.86K ppl 263.63 gN 5.87 bleu 0.00, Fri Jun  1 23:14:25 2018\n",
      "  step 700 lr 1 step-time 3.99s wps 0.86K ppl 189.11 gN 5.36 bleu 0.00, Fri Jun  1 23:21:04 2018\n",
      "  step 800 lr 1 step-time 3.95s wps 0.85K ppl 146.51 gN 4.87 bleu 0.00, Fri Jun  1 23:27:39 2018\n",
      "  step 900 lr 1 step-time 3.95s wps 0.86K ppl 105.29 gN 6.20 bleu 0.00, Fri Jun  1 23:34:14 2018\n",
      "  step 1000 lr 1 step-time 3.99s wps 0.85K ppl 60.88 gN 6.87 bleu 0.00, Fri Jun  1 23:40:53 2018\n",
      "# Save eval, global step 1000\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-1000\n",
      "  loaded infer model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-1000, time 1.28s\n",
      "  # 40\n",
      "    src: oh , darling ! do n't let spoil everything .\n",
      "    ref: oh , darling ! do n't let 's spoil everything .\n",
      "    nmt: oh , dad ! do n't let his father .\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-1000\n",
      "  loaded eval model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-1000, time 0.31s\n",
      "  eval dev: perplexity 27.22, time 4s, Fri Jun  1 23:41:11 2018.\n",
      "  eval test: perplexity 45.79, time 4s, Fri Jun  1 23:41:15 2018.\n",
      "  step 1100 lr 1 step-time 4.05s wps 0.85K ppl 28.10 gN 7.58 bleu 0.00, Fri Jun  1 23:48:00 2018\n",
      "  step 1200 lr 1 step-time 4.03s wps 0.86K ppl 14.99 gN 7.97 bleu 0.00, Fri Jun  1 23:54:42 2018\n",
      "  step 1300 lr 1 step-time 4.05s wps 0.86K ppl 10.44 gN 8.14 bleu 0.00, Sat Jun  2 00:01:27 2018\n",
      "  step 1400 lr 1 step-time 4.02s wps 0.86K ppl 7.87 gN 8.74 bleu 0.00, Sat Jun  2 00:08:09 2018\n",
      "  step 1500 lr 1 step-time 3.98s wps 0.86K ppl 6.98 gN 9.42 bleu 0.00, Sat Jun  2 00:14:47 2018\n",
      "  step 1600 lr 1 step-time 4.09s wps 0.87K ppl 5.94 gN 8.08 bleu 0.00, Sat Jun  2 00:21:35 2018\n",
      "  step 1700 lr 1 step-time 4.10s wps 0.86K ppl 5.51 gN 9.13 bleu 0.00, Sat Jun  2 00:28:25 2018\n",
      "  step 1800 lr 1 step-time 3.97s wps 0.86K ppl 4.53 gN 9.64 bleu 0.00, Sat Jun  2 00:35:02 2018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 1900 lr 1 step-time 4.05s wps 0.85K ppl 4.06 gN 8.76 bleu 0.00, Sat Jun  2 00:41:47 2018\n",
      "  step 2000 lr 1 step-time 4.00s wps 0.86K ppl 4.49 gN 25.35 bleu 0.00, Sat Jun  2 00:48:28 2018\n",
      "# Save eval, global step 2000\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-2000\n",
      "  loaded infer model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-2000, time 0.38s\n",
      "  # 130\n",
      "    src: why did you do that ?\n",
      "    ref: why did you do that ?\n",
      "    nmt: why did you do that ?\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-2000\n",
      "  loaded eval model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-2000, time 0.16s\n",
      "  eval dev: perplexity 3.61, time 4s, Sat Jun  2 00:48:34 2018.\n",
      "  eval test: perplexity 6.60, time 4s, Sat Jun  2 00:48:38 2018.\n",
      "  step 2100 lr 1 step-time 4.03s wps 0.86K ppl 4.30 gN 14.59 bleu 0.00, Sat Jun  2 00:55:21 2018\n",
      "  step 2200 lr 1 step-time 4.13s wps 0.87K ppl 3.79 gN 8.46 bleu 0.00, Sat Jun  2 01:02:14 2018\n",
      "  step 2300 lr 1 step-time 4.01s wps 0.86K ppl 3.28 gN 9.30 bleu 0.00, Sat Jun  2 01:08:55 2018\n",
      "# Finished an epoch, step 2377. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-2000\n",
      "  loaded infer model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-2000, time 0.30s\n",
      "  # 94\n",
      "    src: elizabeth ! ? ! 'ca n't waits any longer . arrive in your arms at ten tonight . ' oh , god ! not tonight .\n",
      "    ref: elizabeth ! ? ! 'ca n't waits any longer . arrive in your arms at ten tonight . ' oh , god ! not tonight .\n",
      "    nmt: nor ! ? ! the grey n't planned any lawyer . the program in your plans at ten tonight . ' oh , god ! not tonight .\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-2000\n",
      "  loaded infer model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-2000, time 0.12s\n",
      "# External evaluation, global step 2000\n",
      "  decoding to output /home/sven/dialog_corpus/nmt_attention_model/output_dev.\n",
      "  done, num sentences 304, num translations per input 1, time 9s, Sat Jun  2 01:14:10 2018.\n",
      "  bleu dev: 52.5\n",
      "  saving hparams to /home/sven/dialog_corpus/nmt_attention_model/hparams\n",
      "# External evaluation, global step 2000\n",
      "  decoding to output /home/sven/dialog_corpus/nmt_attention_model/output_test.\n",
      "  done, num sentences 305, num translations per input 1, time 9s, Sat Jun  2 01:14:20 2018.\n",
      "  bleu test: 43.4\n",
      "  saving hparams to /home/sven/dialog_corpus/nmt_attention_model/hparams\n",
      "  step 2400 lr 1 step-time 4.10s wps 0.81K ppl 3.44 gN 15.93 bleu 52.50, Sat Jun  2 01:16:05 2018\n",
      "  step 2500 lr 1 step-time 3.99s wps 0.86K ppl 3.20 gN 13.79 bleu 52.50, Sat Jun  2 01:22:44 2018\n",
      "  step 2600 lr 1 step-time 4.09s wps 0.86K ppl 3.48 gN 57.36 bleu 52.50, Sat Jun  2 01:29:32 2018\n",
      "  step 2700 lr 1 step-time 3.93s wps 0.86K ppl 3.31 gN 20.85 bleu 52.50, Sat Jun  2 01:36:06 2018\n",
      "  step 2800 lr 1 step-time 3.96s wps 0.86K ppl 2.94 gN 12.62 bleu 52.50, Sat Jun  2 01:42:42 2018\n",
      "  step 2900 lr 1 step-time 3.99s wps 0.86K ppl 2.96 gN 27.41 bleu 52.50, Sat Jun  2 01:49:21 2018\n",
      "  step 3000 lr 1 step-time 4.01s wps 0.86K ppl 2.81 gN 12.75 bleu 52.50, Sat Jun  2 01:56:02 2018\n",
      "# Save eval, global step 3000\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-3000\n",
      "  loaded infer model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-3000, time 0.17s\n",
      "  # 241\n",
      "    src: you ca n't win 'em all .\n",
      "    ref: you ca n't win 'em all .\n",
      "    nmt: you ca n't win 'em all .\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-3000\n",
      "  loaded eval model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-3000, time 0.12s\n",
      "  eval dev: perplexity 2.72, time 4s, Sat Jun  2 01:56:07 2018.\n",
      "  eval test: perplexity 4.23, time 4s, Sat Jun  2 01:56:11 2018.\n",
      "  step 3100 lr 1 step-time 3.97s wps 0.86K ppl 2.81 gN 11.87 bleu 52.50, Sat Jun  2 02:02:49 2018\n",
      "  step 3200 lr 1 step-time 4.00s wps 0.86K ppl 2.61 gN 8.40 bleu 52.50, Sat Jun  2 02:09:29 2018\n",
      "  step 3300 lr 1 step-time 3.96s wps 0.86K ppl 2.52 gN 8.59 bleu 52.50, Sat Jun  2 02:16:05 2018\n",
      "  step 3400 lr 1 step-time 3.99s wps 0.86K ppl 2.79 gN 36.12 bleu 52.50, Sat Jun  2 02:22:44 2018\n",
      "  step 3500 lr 1 step-time 4.01s wps 0.86K ppl 2.66 gN 18.99 bleu 52.50, Sat Jun  2 02:29:26 2018\n",
      "  step 3600 lr 1 step-time 3.99s wps 0.86K ppl 2.46 gN 10.08 bleu 52.50, Sat Jun  2 02:36:05 2018\n",
      "  step 3700 lr 1 step-time 4.07s wps 0.86K ppl 2.50 gN 16.25 bleu 52.50, Sat Jun  2 02:42:52 2018\n",
      "  step 3800 lr 1 step-time 3.98s wps 0.86K ppl 2.32 gN 9.40 bleu 52.50, Sat Jun  2 02:49:30 2018\n",
      "  step 3900 lr 1 step-time 4.06s wps 0.87K ppl 2.26 gN 8.07 bleu 52.50, Sat Jun  2 02:56:16 2018\n",
      "  step 4000 lr 1 step-time 3.97s wps 0.86K ppl 2.30 gN 9.27 bleu 52.50, Sat Jun  2 03:02:53 2018\n",
      "# Save eval, global step 4000\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-4000\n",
      "  loaded infer model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-4000, time 0.18s\n",
      "  # 170\n",
      "    src: my name is pronounced fron kon steen .\n",
      "    ref: my name is pronounced fron kon steen .\n",
      "    nmt: my name is maggie the dreamer characters .\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-4000\n",
      "  loaded eval model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-4000, time 0.14s\n",
      "  eval dev: perplexity 2.21, time 4s, Sat Jun  2 03:02:58 2018.\n",
      "  eval test: perplexity 3.30, time 4s, Sat Jun  2 03:03:03 2018.\n",
      "  step 4100 lr 1 step-time 4.00s wps 0.87K ppl 2.34 gN 59.35 bleu 52.50, Sat Jun  2 03:09:43 2018\n",
      "  step 4200 lr 1 step-time 4.09s wps 0.87K ppl 2.26 gN 16.52 bleu 52.50, Sat Jun  2 03:16:31 2018\n",
      "  step 4300 lr 1 step-time 3.95s wps 0.86K ppl 2.23 gN 10.46 bleu 52.50, Sat Jun  2 03:23:06 2018\n",
      "  step 4400 lr 1 step-time 4.07s wps 0.87K ppl 2.24 gN 12.73 bleu 52.50, Sat Jun  2 03:29:53 2018\n",
      "  step 4500 lr 1 step-time 4.04s wps 0.87K ppl 2.35 gN 16.90 bleu 52.50, Sat Jun  2 03:36:38 2018\n",
      "  step 4600 lr 1 step-time 3.96s wps 0.87K ppl 2.18 gN 14.51 bleu 52.50, Sat Jun  2 03:43:13 2018\n",
      "  step 4700 lr 1 step-time 3.98s wps 0.86K ppl 2.32 gN 50.42 bleu 52.50, Sat Jun  2 03:49:52 2018\n",
      "# Finished an epoch, step 4754. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-4000\n",
      "  loaded infer model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-4000, time 0.47s\n",
      "  # 13\n",
      "    src: mmmmm .\n",
      "    ref: mmmmm .\n",
      "    nmt: peanut .\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-4000\n",
      "  loaded infer model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-4000, time 0.14s\n",
      "# External evaluation, global step 4000\n",
      "  decoding to output /home/sven/dialog_corpus/nmt_attention_model/output_dev.\n",
      "  done, num sentences 304, num translations per input 1, time 7s, Sat Jun  2 03:53:39 2018.\n",
      "  bleu dev: 74.2\n",
      "  saving hparams to /home/sven/dialog_corpus/nmt_attention_model/hparams\n",
      "# External evaluation, global step 4000\n",
      "  decoding to output /home/sven/dialog_corpus/nmt_attention_model/output_test.\n",
      "  done, num sentences 305, num translations per input 1, time 8s, Sat Jun  2 03:53:48 2018.\n",
      "  bleu test: 63.4\n",
      "  saving hparams to /home/sven/dialog_corpus/nmt_attention_model/hparams\n",
      "  step 4800 lr 1 step-time 4.21s wps 0.82K ppl 2.65 gN 189.41 bleu 74.15, Sat Jun  2 03:57:10 2018\n",
      "  step 4900 lr 1 step-time 4.01s wps 0.86K ppl 2.24 gN 24.83 bleu 74.15, Sat Jun  2 04:03:50 2018\n",
      "  step 5000 lr 1 step-time 3.97s wps 0.87K ppl 2.55 gN 135.82 bleu 74.15, Sat Jun  2 04:10:27 2018\n",
      "# Save eval, global step 5000\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-5000\n",
      "  loaded infer model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-5000, time 0.19s\n",
      "  # 241\n",
      "    src: you ca n't win 'em all .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ref: you ca n't win 'em all .\n",
      "    nmt: you ca n't win 'em all .\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-5000\n",
      "  loaded eval model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-5000, time 0.33s\n",
      "  eval dev: perplexity 2.25, time 4s, Sat Jun  2 04:10:33 2018.\n",
      "  eval test: perplexity 3.24, time 5s, Sat Jun  2 04:10:38 2018.\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-5000\n",
      "  loaded infer model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-5000, time 0.13s\n",
      "  # 147\n",
      "    src: but look at what 's been done with hearts and kidneys !\n",
      "    ref: but look at what 's been done with hearts and kidneys !\n",
      "    nmt: but look at what 's been done with picard and wilkins !\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-5000\n",
      "  loaded infer model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-5000, time 0.13s\n",
      "# External evaluation, global step 5000\n",
      "  decoding to output /home/sven/dialog_corpus/nmt_attention_model/output_dev.\n",
      "  done, num sentences 304, num translations per input 1, time 7s, Sat Jun  2 04:10:47 2018.\n",
      "  bleu dev: 75.3\n",
      "  saving hparams to /home/sven/dialog_corpus/nmt_attention_model/hparams\n",
      "# External evaluation, global step 5000\n",
      "  decoding to output /home/sven/dialog_corpus/nmt_attention_model/output_test.\n",
      "  done, num sentences 305, num translations per input 1, time 7s, Sat Jun  2 04:10:55 2018.\n",
      "  bleu test: 66.4\n",
      "  saving hparams to /home/sven/dialog_corpus/nmt_attention_model/hparams\n",
      "  step 5100 lr 1 step-time 4.03s wps 0.87K ppl 2.55 gN 42.78 bleu 75.26, Sat Jun  2 04:17:38 2018\n",
      "  step 5200 lr 1 step-time 3.82s wps 0.85K ppl 2.16 gN 15.41 bleu 75.26, Sat Jun  2 04:24:00 2018\n",
      "  step 5300 lr 1 step-time 3.97s wps 0.86K ppl 2.05 gN 17.26 bleu 75.26, Sat Jun  2 04:30:37 2018\n",
      "  step 5400 lr 1 step-time 4.00s wps 0.86K ppl 2.00 gN 47.05 bleu 75.26, Sat Jun  2 04:37:17 2018\n",
      "  step 5500 lr 1 step-time 4.03s wps 0.87K ppl 2.02 gN 19.07 bleu 75.26, Sat Jun  2 04:44:00 2018\n",
      "  step 5600 lr 1 step-time 3.95s wps 0.86K ppl 1.97 gN 11.58 bleu 75.26, Sat Jun  2 04:50:35 2018\n",
      "  step 5700 lr 1 step-time 3.92s wps 0.85K ppl 1.84 gN 10.47 bleu 75.26, Sat Jun  2 04:57:07 2018\n",
      "  step 5800 lr 1 step-time 3.96s wps 0.87K ppl 1.99 gN 20.08 bleu 75.26, Sat Jun  2 05:03:44 2018\n",
      "  step 5900 lr 1 step-time 3.99s wps 0.86K ppl 2.27 gN 11.38 bleu 75.26, Sat Jun  2 05:10:22 2018\n",
      "  step 6000 lr 1 step-time 4.05s wps 0.86K ppl 2.01 gN 43.88 bleu 75.26, Sat Jun  2 05:17:07 2018\n",
      "# Save eval, global step 6000\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-6000\n",
      "  loaded infer model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-6000, time 0.18s\n",
      "  # 91\n",
      "    src: you have your tickets ?\n",
      "    ref: you have your tickets ?\n",
      "    nmt: you have your tickets ?\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-6000\n",
      "  loaded eval model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-6000, time 0.14s\n",
      "  eval dev: perplexity 1.96, time 4s, Sat Jun  2 05:17:12 2018.\n",
      "  eval test: perplexity 3.10, time 4s, Sat Jun  2 05:17:17 2018.\n",
      "  step 6100 lr 1 step-time 3.93s wps 0.87K ppl 1.99 gN 55.70 bleu 75.26, Sat Jun  2 05:23:50 2018\n",
      "  step 6200 lr 1 step-time 4.05s wps 0.86K ppl 1.89 gN 9.53 bleu 75.26, Sat Jun  2 05:30:36 2018\n",
      "  step 6300 lr 1 step-time 4.00s wps 0.86K ppl 1.79 gN 7.97 bleu 75.26, Sat Jun  2 05:37:16 2018\n",
      "  step 6400 lr 1 step-time 4.05s wps 0.87K ppl 1.92 gN 16.79 bleu 75.26, Sat Jun  2 05:44:01 2018\n",
      "  step 6500 lr 1 step-time 4.04s wps 0.87K ppl 2.13 gN 40.66 bleu 75.26, Sat Jun  2 05:50:45 2018\n",
      "  step 6600 lr 1 step-time 3.99s wps 0.86K ppl 2.04 gN 41.39 bleu 75.26, Sat Jun  2 05:57:24 2018\n",
      "  step 6700 lr 1 step-time 4.05s wps 0.87K ppl 1.98 gN 422.72 bleu 75.26, Sat Jun  2 06:04:09 2018\n",
      "  step 6800 lr 1 step-time 4.02s wps 0.86K ppl 1.79 gN 8.82 bleu 75.26, Sat Jun  2 06:10:51 2018\n",
      "  step 6900 lr 1 step-time 4.03s wps 0.87K ppl 1.88 gN 20.06 bleu 75.26, Sat Jun  2 06:17:34 2018\n",
      "  step 7000 lr 1 step-time 3.99s wps 0.86K ppl 1.85 gN 19.09 bleu 75.26, Sat Jun  2 06:24:13 2018\n",
      "# Save eval, global step 7000\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-7000\n",
      "  loaded infer model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-7000, time 0.17s\n",
      "  # 298\n",
      "    src: did you hear that strange music ?\n",
      "    ref: did you hear that strange music ?\n",
      "    nmt: did you hear that strange music ?\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-7000\n",
      "  loaded eval model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-7000, time 0.15s\n",
      "  eval dev: perplexity 1.80, time 4s, Sat Jun  2 06:24:18 2018.\n",
      "  eval test: perplexity 2.62, time 4s, Sat Jun  2 06:24:23 2018.\n",
      "  step 7100 lr 1 step-time 4.08s wps 0.87K ppl 1.84 gN 11.47 bleu 75.26, Sat Jun  2 06:31:11 2018\n",
      "# Finished an epoch, step 7131. Perform external evaluation\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-7000\n",
      "  loaded infer model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-7000, time 0.39s\n",
      "  # 221\n",
      "    src: but do n't you see ... the brain of hans delbruck is inside this grotesque hulk -- pleading with us . i 've got to untie it .\n",
      "    ref: but do n't you see ... the brain of hans delbruck is inside this grotesque hulk -- pleading with us . i 've got to untie it .\n",
      "    nmt: but do n't you see ... the brain of hans critics is inside this wit dent -- aramis with us . i 've got to thailand it .\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-7000\n",
      "  loaded infer model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-7000, time 0.11s\n",
      "# External evaluation, global step 7000\n",
      "  decoding to output /home/sven/dialog_corpus/nmt_attention_model/output_dev.\n",
      "  done, num sentences 304, num translations per input 1, time 7s, Sat Jun  2 06:33:17 2018.\n",
      "  bleu dev: 81.4\n",
      "  saving hparams to /home/sven/dialog_corpus/nmt_attention_model/hparams\n",
      "# External evaluation, global step 7000\n",
      "  decoding to output /home/sven/dialog_corpus/nmt_attention_model/output_test.\n",
      "  done, num sentences 305, num translations per input 1, time 7s, Sat Jun  2 06:33:25 2018.\n",
      "  bleu test: 72.6\n",
      "  saving hparams to /home/sven/dialog_corpus/nmt_attention_model/hparams\n",
      "  step 7200 lr 1 step-time 4.11s wps 0.81K ppl 1.77 gN 17.86 bleu 81.38, Sat Jun  2 06:38:18 2018\n",
      "  step 7300 lr 1 step-time 4.00s wps 0.86K ppl 1.93 gN 18.54 bleu 81.38, Sat Jun  2 06:44:58 2018\n",
      "  step 7400 lr 1 step-time 3.95s wps 0.86K ppl 1.83 gN 20.20 bleu 81.38, Sat Jun  2 06:51:33 2018\n",
      "  step 7500 lr 1 step-time 3.99s wps 0.86K ppl 1.89 gN 26.23 bleu 81.38, Sat Jun  2 06:58:13 2018\n",
      "  step 7600 lr 1 step-time 4.00s wps 0.87K ppl 1.97 gN 49.51 bleu 81.38, Sat Jun  2 07:04:52 2018\n",
      "  step 7700 lr 1 step-time 4.01s wps 0.86K ppl 1.80 gN 10.37 bleu 81.38, Sat Jun  2 07:11:33 2018\n",
      "  step 7800 lr 1 step-time 3.95s wps 0.86K ppl 1.76 gN 36.46 bleu 81.38, Sat Jun  2 07:18:08 2018\n",
      "  step 7900 lr 1 step-time 3.96s wps 0.86K ppl 1.85 gN 38.71 bleu 81.38, Sat Jun  2 07:24:44 2018\n",
      "  step 8000 lr 1 step-time 4.01s wps 0.87K ppl 2.01 gN 388.87 bleu 81.38, Sat Jun  2 07:31:25 2018\n",
      "# Save eval, global step 8000\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-8000\n",
      "  loaded infer model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-8000, time 0.17s\n",
      "  # 204\n",
      "    src: yes , master !\n",
      "    ref: yes , master !\n",
      "    nmt: yes , armed !\n",
      "INFO:tensorflow:Restoring parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-8000\n",
      "  loaded eval model parameters from /home/sven/dialog_corpus/nmt_attention_model/translate.ckpt-8000, time 0.13s\n",
      "  eval dev: perplexity 2.06, time 4s, Sat Jun  2 07:31:30 2018.\n",
      "  eval test: perplexity 3.23, time 4s, Sat Jun  2 07:31:35 2018.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step 8100 lr 1 step-time 3.98s wps 0.86K ppl 2.01 gN 92.10 bleu 81.38, Sat Jun  2 07:38:12 2018\n",
      "  step 8200 lr 1 step-time 3.95s wps 0.87K ppl 2.12 gN 115.70 bleu 81.38, Sat Jun  2 07:44:48 2018\n",
      "  step 8300 lr 1 step-time 3.97s wps 0.87K ppl 2.33 gN 216.71 bleu 81.38, Sat Jun  2 07:51:25 2018\n",
      "  step 8400 lr 1 step-time 3.97s wps 0.86K ppl 1.95 gN 32.72 bleu 81.38, Sat Jun  2 07:58:02 2018\n",
      "  step 8500 lr 1 step-time 4.04s wps 0.87K ppl 2.07 gN 161.12 bleu 81.38, Sat Jun  2 08:04:46 2018\n"
     ]
    }
   ],
   "source": [
    "train(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so you 're on your way to tir asleen , huh ? i hate to tell you this , willow , but tir asleen dos n't exist .\n",
      "so you 're on your way to tir asleen , huh ? i hate to tell you this , willow , but tir asleen dos n't exist .\n",
      "so you 're on your way to attract livingston , huh ? i hate to tell you this , languages , but contacted livingston g n't exist .\n",
      "\n",
      "i did ?\n",
      "i did ?\n",
      "i did ?\n",
      "\n",
      "madmartigan ! you saved her life !\n",
      "madmartigan ! you saved her life !\n",
      "hooked ! you saved her life !\n",
      "\n",
      "when i left the crossroads , i got ambushed by an elf !\n",
      "when i left the crossroads , i got ambushed by an elf !\n",
      "when i left the needle , i got the languages by an <unk> !\n",
      "\n",
      "elora danan !\n",
      "elora danan !\n",
      "rok discipline !\n",
      "\n",
      "now willow , i know you 're gon na blame me for this but it was n't my fault ... !\n",
      "now willow , i know you 're gon na blame me for this but it was n't my fault ... !\n",
      "now languages , i know you 're gon na blame me for this but it was n't my fault ... !\n",
      "\n",
      "i thought you had her !\n",
      "i thought you had her !\n",
      "i thought you had her !\n",
      "\n",
      "where 's the baby ? ?\n",
      "where 's the baby ? ?\n",
      "where 's the baby ? ?\n",
      "\n",
      "because she sick .\n",
      "because she 's sick .\n",
      "because she sick .\n",
      "\n",
      "because she sick .\n",
      "because she 's sick .\n",
      "because she sick .\n",
      "\n",
      "time to leave .\n",
      "time to leave .\n",
      "time to leave .\n",
      "\n",
      "time to change her diaper .\n",
      "time to change her diaper .\n",
      "time to change her ransom .\n",
      "\n",
      "nursemaid ? !\n",
      "nursemaid ? !\n",
      "kara ? !\n",
      "\n",
      "these pecks make terrible nurse- maids .\n",
      "these pecks make terrible nurse- maids .\n",
      "these rok make terrible `` tapped .\n",
      "\n",
      "hey ! give me that baby !\n",
      "hey ! give me that baby !\n",
      "hey ! give me that baby !\n",
      "\n",
      "where the hell did you come from ?\n",
      "where the hell did you come from ?\n",
      "where the hell did you come from ?\n",
      "\n",
      "you ! !\n",
      "you ! !\n",
      "you ! !\n",
      "\n",
      "do n't worry , willow . she 's in good hands . you have n't made a mistake .\n",
      "do n't worry , willow . she 's in good hands . you have n't made a mistake .\n",
      "do n't worry , ho-jon . she 's in good hands . you have n't made a mistake .\n",
      "\n",
      "here are her diaper rags . and this is her milk bladder . and when she cries -- she 's either hungry or she 's tired -- rock her on your left shoulder . she likes that .\n",
      "here are her diaper rags . and this is her milk bladder . and when she cries -- she 's either hungry or she 's tired -- rock her on your left shoulder . she likes that .\n",
      "here are her languages survived . and this is her crow languages . and when she stubborn -- she 's either -- rock her on your left bugs . she likes that . she likes that . she likes\n",
      "\n",
      "of course ! i promise !\n",
      "of course ! i promise !\n",
      "of course ! i promise !\n",
      "\n",
      "and give her bath everyday , and do n't let her get cold . and keep her diaper changed .\n",
      "and give her a bath everyday , and do n't let her get cold . and keep her diaper changed .\n",
      "and give her bath everyday , and do n't let her own . and keep her languages changed . and keep\n",
      "\n",
      "absolutely !\n",
      "absolutely !\n",
      "absolutely !\n",
      "\n",
      "fresh goat milk .\n",
      "fresh goat 's milk .\n",
      "fresh languages crow .\n",
      "\n",
      "i will .\n",
      "i will .\n",
      "i will .\n",
      "\n",
      "you got ta promise to feed her .\n",
      "you got ta promise to feed her .\n",
      "you got ta promise to feed her .\n",
      "\n",
      "you do n't know anything about ba- bies .\n",
      "you do n't know anything about ba- bies .\n",
      "you do n't know anything about `` <unk> .\n",
      "\n",
      "nobody 's gon na take your baby . know why ? nobody cares ... except me . you wan na go back to your farm . you wan na go back to your family . i could take care of that baby . i 'll look after her like she was me own .\n",
      "nobody 's gon na take your baby . know why ? nobody cares ... except me . you wan na go back to your farm . you wan na go back to your family . i could take care of that baby . i 'll look after her like she was me own .\n",
      "nobody 's gon na take your baby . know why ? except me . you wan na go back to your farm . you wan na go back to your farm . you wan na go back baby . i could after her like she was me own . i 'll look after her like\n",
      "\n",
      "i hate this .\n",
      "i hate this .\n",
      "i hate this .\n",
      "\n",
      "whatcha thinkin ' about , willow ?\n",
      "whatcha thinkin ' about , willow ?\n",
      "twenty-six monday ' about , ho-jon ?\n",
      "\n",
      "friend of yours ?\n",
      "friend of yours ?\n",
      "friend of yours ?\n",
      "\n",
      "i 'll be around long after you 're dead , airk ! you slime ! when i get outa here i 'll cut your head off and stick it on a pig-pole !\n",
      "i 'll be around long after you 're dead , airk ! you slime ! when i get outa here i 'll cut your head off and stick it on a pig-pole !\n",
      "i 'll be around long after you 're dead , narcissus ! you hooked ! when i get outa here i 'll cut your head off and send it on a `` !\n",
      "\n",
      "what is it ?\n",
      "what is it ?\n",
      "what is it ?\n",
      "\n",
      "you hear trouble .\n",
      "you hear trouble .\n",
      "you hear trouble .\n",
      "\n",
      "what 's that ? i hear something !\n",
      "what 's that ? i hear something !\n",
      "what 's that ? i hear something !\n",
      "\n",
      "the greatest swordsman that ever lived .\n",
      "the greatest swordsman that ever lived .\n",
      "the greatest stubbins that ever lived .\n",
      "\n",
      "are you a warrior ?\n",
      "are you a warrior ?\n",
      "are you a sandwich ?\n",
      "\n",
      "smells like a battle .\n",
      "smells like a battle .\n",
      "smells like a battle .\n",
      "\n",
      "these burns . my arms . ouch ...\n",
      "these burns . my arms . ouch ...\n",
      "these burns . my arms . anytime ...\n",
      "\n",
      "you all right ?\n",
      "you all right ?\n",
      "you all right ?\n",
      "\n",
      "farmer ! i knew it ! you 're a vic- tim , willow . yep , you and me : victims of a rotten , corrupt , cor- rupt , rotten world .\n",
      "farmer ! i knew it ! you 're a vic- tim , willow . yep , you and me : victims of a rotten , corrupt , cor- rupt , rotten world .\n",
      "farmer ! i knew it ! you 're a `` waits , ho-jon . yep , you and me : weight of a excellency , hooked , <unk> `` , excellency world .\n",
      "\n",
      "farmer .\n",
      "farmer .\n",
      "farmer .\n",
      "\n",
      "you woodcutter ?\n",
      "you a woodcutter ?\n",
      "you <unk> ?\n",
      "\n",
      "i do n't know .\n",
      "i do n't know .\n",
      "i do n't know .\n",
      "\n",
      "as if you care . i saved your life , peck . those guys woulda killed us ! you wan na give your baby to them ? ? they eat babies !\n",
      "as if you care . i saved your life , peck . those guys woulda killed us ! you wan na give your baby to them ? ? they eat babies !\n",
      "as if you care . i saved your life , allie . those guys woulda killed us ! you wan na give your baby to them ? ? they eat babies !\n",
      "\n",
      "are you okay ?\n",
      "are you okay ?\n",
      "are you okay ?\n",
      "\n",
      "thanks for your help , peck .\n",
      "thanks for your help , peck .\n",
      "thanks for your help , allie .\n",
      "\n",
      "peck ! quick ! douse the fire !\n",
      "peck ! quick ! douse the fire !\n",
      "allie ! quick ! discipline the fire !\n",
      "\n",
      "hey ! somebody 's coming !\n",
      "hey ! somebody 's coming !\n",
      "hey ! somebody 's coming !\n",
      "\n",
      "i have to give this baby to some- body .\n",
      "i have to give this baby to some- body .\n",
      "i have to give this baby to amy body .\n",
      "\n",
      "why .\n",
      "why .\n",
      "why .\n",
      "\n",
      "do any other daikinis ever come by here ?\n",
      "do any other daikinis ever come by here ?\n",
      "do any other languages ever come by here ?\n",
      "\n",
      "miserable pecks .\n",
      "miserable pecks .\n",
      "epps livingston .\n",
      "\n",
      "you be careful . i 'm a powerful sorcerer . i could turn you into a toad just like that .\n",
      "you be careful . i 'm a powerful sorcerer . i could turn you into a toad just like that .\n",
      "you be careful . i 'm a dozen roston . i could turn you into a mortgage just like that .\n",
      "\n",
      "do n't make me angry , peck .\n",
      "do n't make me angry , peck .\n",
      "do n't make me angry , allie .\n",
      "\n",
      "good . how long will it take ?\n",
      "good . how long will it take ?\n",
      "good . how long will it take ?\n",
      "\n",
      "please ? i 'm dyin ' of thirst in here .\n",
      "please ? i 'm dyin ' of thirst in here .\n",
      "please ? i 'm dyin ' of the biggest in here .\n",
      "\n",
      "what a great son you are , ranon . i wish i could take you with me . now go to sleep ...\n",
      "what a great son you are , ranon . i wish i could take you with me . now go to sleep ...\n",
      "what a great son you are , <unk> . i wish i could take you with me . now go to sleep ...\n",
      "\n",
      "i could be you rguard ! i could carry your spear !\n",
      "i could be you rguard ! i could carry your spear !\n",
      "i could be you `` ! i could carry your cellar !\n",
      "\n",
      "are you scared ?\n",
      "are you scared ?\n",
      "are you scared ?\n",
      "\n",
      "daikinis are giant people who live far far away .\n",
      "daikinis are giant people who live far far away .\n",
      "organize are helen people who live far far away .\n",
      "\n",
      "dada , what 's a daikini ?\n",
      "dada , what 's a daikini ?\n",
      "harcourt , what 's a countdown ?\n",
      "\n",
      "absolutely not .\n",
      "absolutely not .\n",
      "absolutely not .\n",
      "\n",
      "ca n't we keep it , dada ?\n",
      "ca n't we keep it , dada ?\n",
      "ca n't we keep it , brigman ?\n",
      "\n",
      "no , it 's too big to be a nelwyn baby . it looks like daikini ...\n",
      "no , it 's too big to be a nelwyn baby . it looks like a daikini ...\n",
      "no , it 's too big to be a starboard baby . it looks like the languages ...\n",
      "\n",
      "it 's not nelwyn baby .\n",
      "it 's not a nelwyn baby .\n",
      "it 's not starboard baby .\n",
      "\n",
      "courage , willow .\n",
      "courage , willow .\n",
      "ho , but- .\n",
      "\n",
      "patience , willow .\n",
      "patience , willow .\n",
      "tastes , languages .\n",
      "\n",
      "surrender ! !\n",
      "surrender ! !\n",
      "surrender ! !\n",
      "\n",
      "time is running out .\n",
      "time is running out .\n",
      "time is running out .\n",
      "\n",
      "she 's too powerful , raziel\n",
      "she 's too powerful , raziel\n",
      "she 's too dozen , jez\n",
      "\n",
      "no ! i can still defeat bavmorda !\n",
      "no ! i can still defeat bavmorda !\n",
      "no ! i can still jade encounter !\n",
      "\n",
      "i 've come all this way and now elora dana 's going to die !\n",
      "i 've come all this way and now elora dana 's going to die !\n",
      "i 've come all this way and now taping dana 's going to die !\n",
      "\n",
      "just do it !\n",
      "just do it !\n",
      "just do it !\n",
      "\n",
      "why ?\n",
      "why ?\n",
      "why ?\n",
      "\n",
      "get down ! the ritual chant ! quick ! protect yourself !\n",
      "get down ! the ritual chant ! quick ! protect yourself !\n",
      "get down ! the languages channing ! quick ! protect yourself !\n",
      "\n",
      "you did n't transform them , willow , you released them . that was noth- ing . now change me .\n",
      "you did n't transform them , willow , you released them . that was noth- ing . now change me .\n",
      "you did n't haul them , languages , you fate them . that change me . that was deceive hints .\n",
      "\n",
      "raziel ! i transformed them all ! i 'm a magician !\n",
      "raziel ! i transformed them all ! i 'm a magician !\n",
      "huey ! i 'm taping them all ! i 'm a unicorn !\n",
      "\n",
      "more ! more !\n",
      "more ! more !\n",
      "more ! more !\n",
      "\n",
      "i did it ! madmartigan ! come here ! look ! you got ta see this ! i did it !\n",
      "i did it ! madmartigan ! come here ! look ! you got ta see this ! i did it !\n",
      "i did it ! but- ! come here ! look ! you got ta see this ! i did it !\n",
      "\n",
      "use the philosopher 's stone ! hur- ry ! invocation ! release them !\n",
      "use the philosopher 's stone ! hur- ry ! invocation ! release them !\n",
      "use the engine 's stone ! <unk> thirty-six ! <unk> ! release them !\n",
      "\n",
      "how ? ?\n",
      "how ? ?\n",
      "how ? ?\n",
      "\n",
      "there 's nobody here .\n",
      "there 's nobody here .\n",
      "there 's nobody here .\n",
      "\n",
      "tir asleen .\n",
      "tir asleen .\n",
      "harcourt livingston .\n",
      "\n",
      "no ! change me back into a sor- ceress , willow .\n",
      "no ! change me back into a sor- ceress , willow .\n",
      "no ! change me back into a password mu , but- .\n",
      "\n",
      "i just wanted to test it first .\n",
      "i just wanted to test it first .\n",
      "i just wanted to test it first .\n",
      "\n",
      "now use it .\n",
      "now use it .\n",
      "now use it .\n",
      "\n",
      "his will .\n",
      "his will .\n",
      "his will .\n",
      "\n",
      "again : what is the magician weapon ?\n",
      "again : what is the magician 's weapon ?\n",
      "again : what is the unicorn weapon ?\n",
      "\n",
      "we need blood . to nourish the stone . charge the stone with ener- gy .\n",
      "we need blood . to nourish the stone . charge the stone with ener- gy .\n",
      "we need blood . to discipline the stone . charge the stone with `` <unk> .\n",
      "\n",
      "what 'd you bit me for ! !\n",
      "what 'd you bit me for ! !\n",
      "what 'd you bit me for ! !\n",
      "\n",
      "shhhhh !\n",
      "shhhhh !\n",
      "llewelyn !\n",
      "\n",
      "ouch ! !\n",
      "ouch ! !\n",
      "ouch ! !\n",
      "\n",
      "that the life spark . exxence of magic and sorcery !\n",
      "that 's the life spark . exxence of magic and sorcery !\n",
      "that 's the life tapped . `` of magic and but- !\n",
      "\n",
      "oooh , this smells terrible .\n",
      "oooh , this smells terrible .\n",
      "oooh , this smells terrible .\n",
      "\n",
      "me !\n",
      "me !\n",
      "me !\n",
      "\n",
      "but what am i gon na transform ? ?\n",
      "but what am i gon na transform ? ?\n",
      "but what am i gon na snore ? ?\n",
      "\n",
      "art of transformation !\n",
      "the art of transformation !\n",
      "art of bart !\n",
      "\n",
      "i ca n't remember all these things . what am i learning ?\n",
      "i ca n't remember all these things . what am i learning ?\n",
      "i ca n't remember all these things . what am i learning ?\n",
      "\n",
      "your will ! your will ! what else could it be ?\n",
      "your will ! your will ! what else could it be ?\n",
      "your will ! your will what else could it be ?\n",
      "\n",
      "the limitless power of ...\n",
      "the limitless power of ...\n",
      "the discipline power of ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(test_path, \"r\") as tgt_data, \\\n",
    "        open(perturbed_test_path, \"r\") as src_data, \\\n",
    "        open(os.path.join(root_data_path, \"nmt_attention_model/output_test\"), \"r\") as nmt_data:\n",
    "    for i in range(100):\n",
    "        print(src_data.readline().strip())\n",
    "        print(tgt_data.readline().strip())\n",
    "        print(nmt_data.readline().strip())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reader = MovieDialogReader(config, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data; train = C:\\Users\\svenk\\Documents\\dialog_corpus\\train.txt, test = C:\\Users\\svenk\\Documents\\dialog_corpus\\val.txt\n",
      "Creating 4 layers of 512 units.\n",
      "Reading model parameters from C:\\Users\\svenk\\Documents\\dialog_corpus\\dialog_correcter_model_testnltk\\translate.ckpt-10800\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\svenk\\Documents\\dialog_corpus\\dialog_correcter_model_testnltk\\translate.ckpt-10800\n",
      "Training bucket sizes: [198232, 85878, 50002, 70314]\n",
      "Total train size: 404426.0\n",
      "global step 10900 learning rate 0.3850 step-time 6.70 perplexity 21.27\n",
      "  eval: bucket 0 perplexity 3.61\n",
      "  eval: bucket 1 perplexity 40.10\n",
      "  eval: bucket 2 perplexity 120.03\n",
      "  eval: bucket 3 perplexity 175.38\n",
      "global step 11000 learning rate 0.3850 step-time 5.66 perplexity 21.15\n",
      "  eval: bucket 0 perplexity 7.59\n",
      "  eval: bucket 1 perplexity 71.84\n",
      "  eval: bucket 2 perplexity 172.14\n",
      "  eval: bucket 3 perplexity 361.24\n",
      "global step 11100 learning rate 0.3850 step-time 5.85 perplexity 29.12\n",
      "  eval: bucket 0 perplexity 4.33\n",
      "  eval: bucket 1 perplexity 45.23\n",
      "  eval: bucket 2 perplexity 126.60\n",
      "  eval: bucket 3 perplexity 304.59\n",
      "global step 11200 learning rate 0.3850 step-time 6.09 perplexity 20.04\n",
      "  eval: bucket 0 perplexity 3.48\n",
      "  eval: bucket 1 perplexity 29.61\n",
      "  eval: bucket 2 perplexity 57.16\n",
      "  eval: bucket 3 perplexity 120.22\n",
      "global step 11300 learning rate 0.3850 step-time 6.72 perplexity 19.86\n",
      "  eval: bucket 0 perplexity 3.73\n",
      "  eval: bucket 1 perplexity 24.58\n",
      "  eval: bucket 2 perplexity 43.68\n",
      "  eval: bucket 3 perplexity 54.12\n",
      "global step 11400 learning rate 0.3850 step-time 5.34 perplexity 8.14\n",
      "  eval: bucket 0 perplexity 2.62\n",
      "  eval: bucket 1 perplexity 16.19\n",
      "  eval: bucket 2 perplexity 26.28\n",
      "  eval: bucket 3 perplexity 46.13\n",
      "global step 11500 learning rate 0.3850 step-time 6.00 perplexity 7.34\n",
      "  eval: bucket 0 perplexity 2.40\n",
      "  eval: bucket 1 perplexity 9.69\n",
      "  eval: bucket 2 perplexity 15.34\n",
      "  eval: bucket 3 perplexity 40.36\n",
      "global step 11600 learning rate 0.3850 step-time 6.24 perplexity 6.43\n",
      "  eval: bucket 0 perplexity 2.44\n",
      "  eval: bucket 1 perplexity 9.32\n",
      "  eval: bucket 2 perplexity 18.95\n",
      "  eval: bucket 3 perplexity 36.34\n",
      "global step 11700 learning rate 0.3850 step-time 6.13 perplexity 8.40\n",
      "  eval: bucket 0 perplexity 2.89\n",
      "  eval: bucket 1 perplexity 33.45\n",
      "  eval: bucket 2 perplexity 87.12\n",
      "  eval: bucket 3 perplexity 108.60\n",
      "global step 11800 learning rate 0.3812 step-time 6.64 perplexity 21.24\n",
      "  eval: bucket 0 perplexity 3.06\n",
      "  eval: bucket 1 perplexity 39.98\n",
      "  eval: bucket 2 perplexity 152.25\n",
      "  eval: bucket 3 perplexity 731.00\n",
      "global step 11900 learning rate 0.3774 step-time 5.50 perplexity 47.08\n",
      "  eval: bucket 0 perplexity 9.70\n",
      "  eval: bucket 1 perplexity 348.99\n",
      "  eval: bucket 2 perplexity 3023.10\n",
      "  eval: bucket 3 perplexity 17174.54\n",
      "global step 12000 learning rate 0.3736 step-time 6.08 perplexity 42.45\n",
      "  eval: bucket 0 perplexity 3.47\n",
      "  eval: bucket 1 perplexity 33.01\n",
      "  eval: bucket 2 perplexity 120.12\n",
      "  eval: bucket 3 perplexity 735.40\n",
      "global step 12100 learning rate 0.3736 step-time 5.89 perplexity 39.09\n",
      "  eval: bucket 0 perplexity 2.60\n",
      "  eval: bucket 1 perplexity 74.94\n",
      "  eval: bucket 2 perplexity 443.05\n",
      "  eval: bucket 3 perplexity 9173.13\n",
      "global step 12200 learning rate 0.3736 step-time 5.48 perplexity 28.06\n",
      "  eval: bucket 0 perplexity 3.35\n",
      "  eval: bucket 1 perplexity 53.92\n",
      "  eval: bucket 2 perplexity 267.81\n",
      "  eval: bucket 3 perplexity 797.93\n",
      "global step 12300 learning rate 0.3736 step-time 6.38 perplexity 61.32\n",
      "  eval: bucket 0 perplexity 5.36\n",
      "  eval: bucket 1 perplexity 43.80\n",
      "  eval: bucket 2 perplexity 333.50\n",
      "  eval: bucket 3 perplexity 1626.62\n",
      "global step 12400 learning rate 0.3699 step-time 5.85 perplexity 31.75\n",
      "  eval: bucket 0 perplexity 2.73\n",
      "  eval: bucket 1 perplexity 52.87\n",
      "  eval: bucket 2 perplexity 238.54\n",
      "  eval: bucket 3 perplexity 1275.90\n",
      "global step 12500 learning rate 0.3699 step-time 5.80 perplexity 19.80\n",
      "  eval: bucket 0 perplexity 2.57\n",
      "  eval: bucket 1 perplexity 39.11\n",
      "  eval: bucket 2 perplexity 151.77\n",
      "  eval: bucket 3 perplexity 362.67\n",
      "global step 12600 learning rate 0.3699 step-time 6.58 perplexity 25.76\n",
      "  eval: bucket 0 perplexity 2.88\n",
      "  eval: bucket 1 perplexity 73.90\n",
      "  eval: bucket 2 perplexity 232.76\n",
      "  eval: bucket 3 perplexity 729.92\n",
      "global step 12700 learning rate 0.3699 step-time 6.29 perplexity 27.68\n",
      "  eval: bucket 0 perplexity 3.82\n",
      "  eval: bucket 1 perplexity 41.95\n",
      "  eval: bucket 2 perplexity 140.44\n",
      "  eval: bucket 3 perplexity 392.91\n",
      "global step 12800 learning rate 0.3699 step-time 5.31 perplexity 18.94\n",
      "  eval: bucket 0 perplexity 5.26\n",
      "  eval: bucket 1 perplexity 68.00\n",
      "  eval: bucket 2 perplexity 189.67\n",
      "  eval: bucket 3 perplexity 525.96\n",
      "global step 12900 learning rate 0.3699 step-time 6.24 perplexity 28.09\n",
      "  eval: bucket 0 perplexity 5.31\n",
      "  eval: bucket 1 perplexity 76.46\n",
      "  eval: bucket 2 perplexity 342.89\n",
      "  eval: bucket 3 perplexity 943.03\n",
      "global step 13000 learning rate 0.3662 step-time 5.23 perplexity 34.54\n",
      "  eval: bucket 0 perplexity 5.51\n",
      "  eval: bucket 1 perplexity 74.28\n",
      "  eval: bucket 2 perplexity 217.17\n",
      "  eval: bucket 3 perplexity 563.75\n",
      "global step 13100 learning rate 0.3625 step-time 5.86 perplexity 27.83\n",
      "  eval: bucket 0 perplexity 5.51\n",
      "  eval: bucket 1 perplexity 59.51\n",
      "  eval: bucket 2 perplexity 182.15\n",
      "  eval: bucket 3 perplexity 780.95\n",
      "global step 13200 learning rate 0.3625 step-time 5.10 perplexity 15.35\n",
      "  eval: bucket 0 perplexity 4.37\n",
      "  eval: bucket 1 perplexity 33.51\n",
      "  eval: bucket 2 perplexity 151.38\n",
      "  eval: bucket 3 perplexity 304.97\n",
      "global step 13300 learning rate 0.3625 step-time 6.10 perplexity 20.32\n",
      "  eval: bucket 0 perplexity 3.48\n",
      "  eval: bucket 1 perplexity 30.98\n",
      "  eval: bucket 2 perplexity 71.65\n",
      "  eval: bucket 3 perplexity 227.54\n",
      "global step 13400 learning rate 0.3625 step-time 6.14 perplexity 21.64\n",
      "  eval: bucket 0 perplexity 4.02\n",
      "  eval: bucket 1 perplexity 41.18\n",
      "  eval: bucket 2 perplexity 108.92\n",
      "  eval: bucket 3 perplexity 409.99\n",
      "global step 13500 learning rate 0.3625 step-time 6.01 perplexity 30.02\n",
      "  eval: bucket 0 perplexity 4.56\n",
      "  eval: bucket 1 perplexity 47.80\n",
      "  eval: bucket 2 perplexity 158.42\n",
      "  eval: bucket 3 perplexity 508.35\n",
      "global step 13600 learning rate 0.3589 step-time 6.03 perplexity 32.28\n",
      "  eval: bucket 0 perplexity 6.53\n",
      "  eval: bucket 1 perplexity 46.84\n",
      "  eval: bucket 2 perplexity 121.52\n",
      "  eval: bucket 3 perplexity 238.99\n",
      "global step 13700 learning rate 0.3553 step-time 5.60 perplexity 25.29\n",
      "  eval: bucket 0 perplexity 9.77\n",
      "  eval: bucket 1 perplexity 35.66\n",
      "  eval: bucket 2 perplexity 65.10\n",
      "  eval: bucket 3 perplexity 91.45\n",
      "global step 13800 learning rate 0.3553 step-time 5.89 perplexity 17.59\n",
      "  eval: bucket 0 perplexity 3.96\n",
      "  eval: bucket 1 perplexity 30.96\n",
      "  eval: bucket 2 perplexity 60.59\n",
      "  eval: bucket 3 perplexity 95.63\n",
      "global step 13900 learning rate 0.3553 step-time 4.91 perplexity 12.64\n",
      "  eval: bucket 0 perplexity 6.07\n",
      "  eval: bucket 1 perplexity 33.65\n",
      "  eval: bucket 2 perplexity 54.98\n",
      "  eval: bucket 3 perplexity 91.18\n",
      "global step 14000 learning rate 0.3553 step-time 6.13 perplexity 21.50\n",
      "  eval: bucket 0 perplexity 5.24\n",
      "  eval: bucket 1 perplexity 41.95\n",
      "  eval: bucket 2 perplexity 75.09\n",
      "  eval: bucket 3 perplexity 131.49\n",
      "global step 14100 learning rate 0.3553 step-time 5.57 perplexity 20.40\n",
      "  eval: bucket 0 perplexity 4.83\n",
      "  eval: bucket 1 perplexity 37.43\n",
      "  eval: bucket 2 perplexity 81.29\n",
      "  eval: bucket 3 perplexity 139.56\n",
      "global step 14200 learning rate 0.3553 step-time 6.33 perplexity 16.89\n",
      "  eval: bucket 0 perplexity 3.93\n",
      "  eval: bucket 1 perplexity 37.84\n",
      "  eval: bucket 2 perplexity 68.49\n",
      "  eval: bucket 3 perplexity 90.94\n",
      "global step 14300 learning rate 0.3553 step-time 6.31 perplexity 21.56\n",
      "  eval: bucket 0 perplexity 5.41\n",
      "  eval: bucket 1 perplexity 41.56\n",
      "  eval: bucket 2 perplexity 131.66\n",
      "  eval: bucket 3 perplexity 234.86\n",
      "global step 14400 learning rate 0.3517 step-time 6.09 perplexity 18.87\n",
      "  eval: bucket 0 perplexity 3.56\n",
      "  eval: bucket 1 perplexity 34.58\n",
      "  eval: bucket 2 perplexity 76.72\n",
      "  eval: bucket 3 perplexity 156.07\n",
      "global step 14500 learning rate 0.3517 step-time 5.97 perplexity 13.69\n",
      "  eval: bucket 0 perplexity 4.12\n",
      "  eval: bucket 1 perplexity 30.03\n",
      "  eval: bucket 2 perplexity 105.04\n",
      "  eval: bucket 3 perplexity 284.10\n",
      "global step 14600 learning rate 0.3517 step-time 6.62 perplexity 18.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  eval: bucket 0 perplexity 2.77\n",
      "  eval: bucket 1 perplexity 21.29\n",
      "  eval: bucket 2 perplexity 70.76\n",
      "  eval: bucket 3 perplexity 139.60\n",
      "global step 14700 learning rate 0.3517 step-time 5.98 perplexity 9.24\n",
      "  eval: bucket 0 perplexity 2.55\n",
      "  eval: bucket 1 perplexity 25.23\n",
      "  eval: bucket 2 perplexity 43.40\n",
      "  eval: bucket 3 perplexity 81.04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-6d7a3268a1b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_reader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\deep-code-corrector\\correct_text.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(data_reader, train_path, test_path, model_path)\u001b[0m\n\u001b[0;32m    180\u001b[0m                 train_data, bucket_id)\n\u001b[0;32m    181\u001b[0m             _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n\u001b[1;32m--> 182\u001b[1;33m                                          target_weights, bucket_id, False)\n\u001b[0m\u001b[0;32m    183\u001b[0m             \u001b[0mstep_time\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                 \u001b[1;33m.\u001b[0m\u001b[0msteps_per_checkpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\deep-code-corrector\\text_corrector_models.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, session, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only, corrective_tokens)\u001b[0m\n\u001b[0;32m    286\u001b[0m                 \u001b[0moutput_feed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_feed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mforward_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[1;31m# Gradient norm, loss, no outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train(data_reader, train_path, val_path, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reader = MovieDialogReader(config, train_path, dropout_prob=0.25, replacement_prob=0.25, dataset_copies=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrective_tokens = get_corrective_tokens(data_reader, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(root_data_path, \"corrective_tokens.pickle\"), \"wb\") as f:\n",
    "    pickle.dump(corrective_tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(root_data_path, \"token_to_id.pickle\"), \"wb\") as f:\n",
    "    pickle.dump(data_reader.token_to_id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from C:\\Users\\svenk\\Documents\\dialog_corpus\\dialog_correcter_model_testnltk\\translate.ckpt-14700\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\svenk\\Documents\\dialog_corpus\\dialog_correcter_model_testnltk\\translate.ckpt-14700\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = create_model(sess, True, model_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: you must have girlfriend\n",
      "Output: you must an an\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test a sample from the test dataset.\n",
    "decoded = decode_sentence(sess, model, data_reader, \"you must have girlfriend\", corrective_tokens=corrective_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'must', 'have', 'the', 'must', 'have']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: did n't you say that they 're going to develop this revolutionary new thing ...\n",
      "Output: did you 're to develop to revolutionary ... you 're to UNK ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded = decode_sentence(sess, model, data_reader,\n",
    "                          \"did n't you say that they 're going to develop this revolutionary new thing ...\",\n",
    "                          corrective_tokens=corrective_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kvothe', 'went', 'to', 'UNK']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(sess, model, data_reader, \"kvothe went to market\", corrective_tokens=corrective_tokens, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blablahblah', 'and', 'and', 'bladdddd', 'to', 'UNK']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(sess, model, data_reader, \"blablahblah and bladdddd went to market\", corrective_tokens=corrective_tokens,\n",
    "                verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do', 'you', 'have']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(sess, model, data_reader, \"do you have book\", corrective_tokens=corrective_tokens, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'cardinals', 'did', 'cubs', 'UNK', 'the', 'the', 'UNK', 'UNK']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(sess, model, data_reader, \"the cardinals did better then the cubs\", corrective_tokens=corrective_tokens, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-3ac453e69476>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 4 layers, 40k steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_reader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrective_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, max_samples=1000)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\deep-code-corrector\\correct_text.py\u001b[0m in \u001b[0;36mevaluate_accuracy\u001b[1;34m(sess, model, data_reader, corrective_tokens, test_path, max_samples)\u001b[0m\n\u001b[0;32m    354\u001b[0m         decoding = next(\n\u001b[0;32m    355\u001b[0m             decode(sess, model, data_reader, [source],\n\u001b[1;32m--> 356\u001b[1;33m                    corrective_tokens=corrective_tokens, verbose=False))\n\u001b[0m\u001b[0;32m    357\u001b[0m         \u001b[0mmodel_hypotheses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdecoding\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\deep-code-corrector\\correct_text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(sess, model, data_reader, data_to_decode, corrective_tokens, verbose)\u001b[0m\n\u001b[0;32m    274\u001b[0m         _, _, output_logits = model.step(\n\u001b[0;32m    275\u001b[0m             \u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbucket_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m             True, corrective_tokens=corrective_tokens_mask)\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         oov_input_tokens = [token for token in tokens if\n",
      "\u001b[1;32m~\\deep-code-corrector\\text_corrector_models.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, session, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only, corrective_tokens)\u001b[0m\n\u001b[0;32m    286\u001b[0m                 \u001b[0moutput_feed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_feed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mforward_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[1;31m# Gradient norm, loss, no outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4 layers, 40k steps\n",
    "errors = evaluate_accuracy(sess, model, data_reader, corrective_tokens, test_path)#, max_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 0: (10, 10)\n",
      "\tBaseline BLEU = 0.8368\n",
      "\tModel BLEU = 0.8425\n",
      "\tBaseline Accuracy: 0.9110\n",
      "\tModel Accuracy: 0.9303\n",
      "Bucket 1: (15, 15)\n",
      "\tBaseline BLEU = 0.8818\n",
      "\tModel BLEU = 0.8459\n",
      "\tBaseline Accuracy: 0.8063\n",
      "\tModel Accuracy: 0.8014\n",
      "Bucket 2: (20, 20)\n",
      "\tBaseline BLEU = 0.8891\n",
      "\tModel BLEU = 0.7986\n",
      "\tBaseline Accuracy: 0.7309\n",
      "\tModel Accuracy: 0.6281\n",
      "Bucket 3: (40, 40)\n",
      "\tBaseline BLEU = 0.9099\n",
      "\tModel BLEU = 0.5997\n",
      "\tBaseline Accuracy: 0.6007\n",
      "\tModel Accuracy: 0.1607\n"
     ]
    }
   ],
   "source": [
    "# 4 layers, 30k steps\n",
    "errors = evaluate_accuracy(sess, model, data_reader, corrective_tokens, test_path)#, max_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 0: (10, 10)\n",
      "\tBaseline BLEU = 0.8330\n",
      "\tModel BLEU = 0.8335\n",
      "\tBaseline Accuracy: 0.9067\n",
      "\tModel Accuracy: 0.9218\n",
      "Bucket 1: (15, 15)\n",
      "\tBaseline BLEU = 0.8772\n",
      "\tModel BLEU = 0.8100\n",
      "\tBaseline Accuracy: 0.7980\n",
      "\tModel Accuracy: 0.7437\n",
      "Bucket 2: (20, 20)\n",
      "\tBaseline BLEU = 0.8898\n",
      "\tModel BLEU = 0.7636\n",
      "\tBaseline Accuracy: 0.7366\n",
      "\tModel Accuracy: 0.5370\n",
      "Bucket 3: (40, 40)\n",
      "\tBaseline BLEU = 0.9098\n",
      "\tModel BLEU = 0.5387\n",
      "\tBaseline Accuracy: 0.6041\n",
      "\tModel Accuracy: 0.1117\n"
     ]
    }
   ],
   "source": [
    "# 4 layers, 20k steps\n",
    "errors = evaluate_accuracy(sess, model, data_reader, corrective_tokens, test_path)#, max_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 0: (10, 10)\n",
      "\tBaseline BLEU = 0.8341\n",
      "\tModel BLEU = 0.8516\n",
      "\tBaseline Accuracy: 0.9083\n",
      "\tModel Accuracy: 0.9384\n",
      "Bucket 1: (15, 15)\n",
      "\tBaseline BLEU = 0.8850\n",
      "\tModel BLEU = 0.8860\n",
      "\tBaseline Accuracy: 0.8156\n",
      "\tModel Accuracy: 0.8491\n",
      "Bucket 2: (20, 20)\n",
      "\tBaseline BLEU = 0.8876\n",
      "\tModel BLEU = 0.8880\n",
      "\tBaseline Accuracy: 0.7291\n",
      "\tModel Accuracy: 0.7817\n",
      "Bucket 3: (40, 40)\n",
      "\tBaseline BLEU = 0.9099\n",
      "\tModel BLEU = 0.9045\n",
      "\tBaseline Accuracy: 0.6073\n",
      "\tModel Accuracy: 0.6425\n"
     ]
    }
   ],
   "source": [
    "errors = evaluate_accuracy(sess, model, data_reader, corrective_tokens, test_path)#, max_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding: you beg for mercy in a second .\n",
      "Target:   you 'll beg for mercy in a second .\n",
      "\n",
      "Decoding: i 'm dying for a shower . you could use the one too . and we 'd better check that bandage .\n",
      "Target:   i 'm dying for a shower . you could use one too . and we 'd better check that bandage .\n",
      "\n",
      "Decoding: whatever ... they 've become hotshot computer guys so they get a job to build el computer grande ... skynet ... for the government . right ?\n",
      "Target:   whatever ... they become the hotshot computer guys so they get the job to build el computer grande ... skynet ... for the government . right ?\n",
      "\n",
      "Decoding: did n't you say that they 're going to develop this revolutionary a new thing ...\n",
      "Target:   did n't you say that they 're going to develop this revolutionary new thing ...\n",
      "\n",
      "Decoding: bag some z ?\n",
      "Target:   bag some z 's ?\n",
      "\n",
      "Decoding: sleep . it 'll be a light soon .\n",
      "Target:   sleep . it 'll be light soon .\n",
      "\n",
      "Decoding: well , at least i know what to name him . i do n't suppose you 'd know who father is ? so i do n't tell him to get lost when i meet him .\n",
      "Target:   well , at least i know what to name him . i do n't suppose you 'd know who the father is ? so i do n't tell him to get lost when i meet him .\n",
      "\n",
      "Decoding: we got ta get you to doctor .\n",
      "Target:   we got ta get you to a doctor .\n",
      "\n",
      "Decoding: hunter killers . patrol machines . a build in automated factories . most of us were rounded up , put in camps ... for orderly disposal .\n",
      "Target:   hunter killers . patrol machines . build in automated factories . most of us were rounded up , put in camps ... for orderly disposal .\n",
      "\n",
      "Decoding: but outside , it 's a living human tissue . flesh , skin , hair ... blood . grown for the cyborgs .\n",
      "Target:   but outside , it 's living human tissue . flesh , skin , hair ... blood . grown for the cyborgs .\n",
      "\n",
      "Decoding: you heard enough . decide . are you going to release me ?\n",
      "Target:   you 've heard enough . decide . are you going to release me ?\n",
      "\n",
      "Decoding: okay . okay . but this ... cyborg ... if it metal ...\n",
      "Target:   okay . okay . but this ... cyborg ... if it 's metal ...\n",
      "\n",
      "Decoding: you go naked . something about the field generated by living organism . nothing dead will go .\n",
      "Target:   you go naked . something about the field generated by a living organism . nothing dead will go .\n",
      "\n",
      "Decoding: ca n't . nobody goes home . nobody else comes through . it just him and me .\n",
      "Target:   ca n't . nobody goes home . nobody else comes through . it 's just him and me .\n",
      "\n",
      "Decoding: i see . and this ... computer , thinks it can win by killing the mother of its enemy , kill- ing him , in effect , before he is even conceived ? sort of retroactive abortion ?\n",
      "Target:   i see . and this ... computer , thinks it can win by killing the mother of its enemy , kill- ing him , in effect , before he is even conceived ? a sort of retroactive abortion ?\n",
      "\n",
      "Decoding: skynet . a computer defense system built for sac-norad by cyber dynamics . modified series 4800 .\n",
      "Target:   skynet . a computer defense system built for sac-norad by cyber dynamics . a modified series 4800 .\n",
      "\n",
      "Decoding: a year 2027 ?\n",
      "Target:   the year 2027 ?\n",
      "\n",
      "Decoding: with one thirty a second under perry , from '21 to '27 --\n",
      "Target:   with the one thirty second under perry , from '21 to '27 --\n",
      "\n",
      "Decoding: why do n't you just stretch out here and get some sleep . it take your mom 's a good hour to get here from redlands .\n",
      "Target:   why do n't you just stretch out here and get some sleep . it 'll take your mom a good hour to get here from redlands .\n",
      "\n",
      "Decoding: lieutenant , are you sure it them ? maybe i should see the ... bodies .\n",
      "Target:   lieutenant , are you sure it 's them ? maybe i should see the ... bodies .\n",
      "\n",
      "Decoding: i already did . no answer at the door and the apartment manager 's out . i keeping them there .\n",
      "Target:   i already did . no answer at the door and the apartment manager 's out . i 'm keeping them there .\n",
      "\n",
      "Decoding: that stuff two hours cold .\n",
      "Target:   that stuff 's two hours cold .\n",
      "\n",
      "Decoding: you got ta be kidding me . the new guys 'll be short-stroking it over this one . one-day pattern killer .\n",
      "Target:   you got ta be kidding me . the new guys 'll be short-stroking it over this one . a one-day pattern killer .\n",
      "\n",
      "Decoding: give me a short version .\n",
      "Target:   give me the short version .\n",
      "\n",
      "Decoding: because it 's fair . give me the next quarter . if you still feel this way , vote your shares ...\n",
      "Target:   because it 's fair . give me next quarter . if you still feel this way , vote your shares ...\n",
      "\n",
      "Decoding: it 's probably will . in fact , i 'd go so far as to say it 's almost certainly will , in time . why should i settle for that ?\n",
      "Target:   it probably will . in fact , i 'd go so far as to say it almost certainly will , in time . why should i settle for that ?\n",
      "\n",
      "Decoding: stock will turn .\n",
      "Target:   the stock will turn .\n",
      "\n",
      "Decoding: you want to know what it is ? what 's it all about ? john . chapter nine . verse twenty-five .\n",
      "Target:   you want to know what it is ? what it 's all about ? john . chapter nine . verse twenty-five .\n",
      "\n",
      "Decoding: i only mention it because i took a test this afternoon , down on montgomery street .\n",
      "Target:   i only mention it because i took the test this afternoon , down on montgomery street .\n",
      "\n",
      "Decoding: christine ! mister van orton is valued customer ...\n",
      "Target:   christine ! mister van orton is a valued customer ...\n",
      "\n",
      "Decoding: a single ?\n",
      "Target:   single ?\n",
      "\n",
      "Decoding: there 's another gig starting in saudi arabia . i just a walk-on this time though . bit-part .\n",
      "Target:   there 's another gig starting in saudi arabia . i 'm just a walk-on this time though . bit-part .\n",
      "\n",
      "Decoding: no ! you take another step , i shoot ! they 're trying to kill me ...\n",
      "Target:   no ! you take another step , i 'll shoot ! they 're trying to kill me ...\n",
      "\n",
      "Decoding: listen very carefully , i 'm telling the truth ... this is a game . this was all the game .\n",
      "Target:   listen very carefully , i 'm telling the truth ... this is the game . this was all the game .\n",
      "\n",
      "Decoding: that 's gun . that 's ... that 's not automatic . the guard had an automatic ...\n",
      "Target:   that gun . that ... that 's not automatic . the guard had an automatic ...\n",
      "\n",
      "Decoding: take a picture out .\n",
      "Target:   take the picture out .\n",
      "\n",
      "Decoding: yeah . first communion . are n't i little angel ?\n",
      "Target:   yeah . first communion . are n't i a little angel ?\n",
      "\n",
      "Decoding: let me go get some clothes on . we talk , okay ? be right back .\n",
      "Target:   let me go get some clothes on . we 'll talk , okay ? be right back .\n",
      "\n",
      "Decoding: i 'm tired . i 'm sorry , i should go . i 've been enough of nuisance .\n",
      "Target:   i 'm tired . i 'm sorry , i should go . i 've been enough of a nuisance .\n",
      "\n",
      "Decoding: they said five hundred . i said six . they said man in the gray flannel suit . i think i said , you mean the attractive guy in the gray flannel suit ?\n",
      "Target:   they said five hundred . i said six . they said the man in the gray flannel suit . i think i said , you mean the attractive guy in the gray flannel suit ?\n",
      "\n",
      "Decoding: i have a confession to make . someone gave me six-hundred dollars to spill a drinks on you , as a practical joke .\n",
      "Target:   i have a confession to make . someone gave me six-hundred dollars to spill drinks on you , as a practical joke .\n",
      "\n",
      "Decoding: maitre d ' called you christine .\n",
      "Target:   the maitre d ' called you christine .\n",
      "\n",
      "Decoding: i know owner of campton place . i could talk to him in the morning .\n",
      "Target:   i know the owner of campton place . i could talk to him in the morning .\n",
      "\n",
      "Decoding: fresh shirt ...\n",
      "Target:   a fresh shirt ...\n",
      "\n",
      "Decoding: investment banking . moving money from a place to place .\n",
      "Target:   investment banking . moving money from place to place .\n",
      "\n",
      "Decoding: what 's the c .r .s . ?\n",
      "Target:   what 's c .r .s . ?\n",
      "\n",
      "Decoding: this is a c .r .s .\n",
      "Target:   this is c .r .s .\n",
      "\n",
      "Decoding: their ladder here .\n",
      "Target:   there 's a ladder here .\n",
      "\n",
      "Decoding: this is n't attempt to be gallant . if i do n't lift you , how are you going to get there ?\n",
      "Target:   this is n't an attempt to be gallant . if i do n't lift you , how are you going to get there ?\n",
      "\n",
      "Decoding: are you suggesting we wait till someone 's finds us ?\n",
      "Target:   are you suggesting we wait till someone finds us ?\n",
      "\n",
      "Decoding: `` ... wait for help . '' wait for help . i 'm not opening that specifically warns me not to .\n",
      "Target:   `` ... wait for help . '' wait for help . i 'm not opening a door that specifically warns me not to .\n",
      "\n",
      "Decoding: read what it says : `` warning , do < u > not < /u > attempt to open . if elevator stops , use the emergency ... ``\n",
      "Target:   read what it says : `` warning , do < u > not < /u > attempt to open . if elevator stops , use emergency ... ``\n",
      "\n",
      "Decoding: long story . i found this key in the mouth of wooden harlequin .\n",
      "Target:   long story . i found this key in the mouth of a wooden harlequin .\n",
      "\n",
      "Decoding: how do you know that way ?\n",
      "Target:   how do you know that 's the way ?\n",
      "\n",
      "Decoding: it 's run by company ... they play elaborate pranks . things like this . i 'm really only now finding out myself .\n",
      "Target:   it 's run by a company ... they play elaborate pranks . things like this . i 'm really only now finding out myself .\n",
      "\n",
      "Decoding: you got to be kidding .\n",
      "Target:   you 've got to be kidding .\n",
      "\n",
      "Decoding: i do n't think he breathing .\n",
      "Target:   i do n't think he 's breathing .\n",
      "\n",
      "Decoding: a bad month . you did exact the same thing to me last week .\n",
      "Target:   a bad month . you did the exact same thing to me last week .\n",
      "\n",
      "Decoding: yeah , yeah . she 's called a cab . said something about catching plane .\n",
      "Target:   yeah , yeah . she called a cab . said something about catching a plane .\n",
      "\n",
      "Decoding: oh , god yes please . thanks , man . i take you up on that .\n",
      "Target:   oh , god yes please . thanks , man . i 'll take you up on that .\n",
      "\n",
      "Decoding: this ... ? oh , this is just ... this is bill .\n",
      "Target:   this ... ? oh , this is just ... this is the bill .\n",
      "\n",
      "Decoding: baby , they were all over the house with metal detectors . they switched your gun with look-alike , rigged barrel , loaded with blanks . pop-gun .\n",
      "Target:   baby , they were all over the house with metal detectors . they switched your gun with a look-alike , rigged barrel , loaded with blanks . pop-gun .\n",
      "\n",
      "Decoding: you dodged bullet .\n",
      "Target:   you dodged a bullet .\n",
      "\n",
      "Decoding: c .r .s . who do you think ? jesus h . , thank your lucky charms . to think what i 've almost got you into .\n",
      "Target:   c .r .s . who do you think ? jesus h . , thank your lucky charms . to think what i almost got you into .\n",
      "\n",
      "Decoding: it 's profound life experience .\n",
      "Target:   it 's a profound life experience .\n",
      "\n",
      "Decoding: you 've heard of it . you 've seen other people having it . they 're entertainment service , but more than that .\n",
      "Target:   you 've heard of it . you 've seen other people having it . they 're an entertainment service , but more than that .\n",
      "\n",
      "Decoding: they make your life fun . there 's only guarantee is you will not be bored .\n",
      "Target:   they make your life fun . their only guarantee is you will not be bored .\n",
      "\n",
      "Decoding: not after i done with it . actually , i 've been here . in grad-school i bought crystal-meth from the maitre d ' .\n",
      "Target:   not after i 'm done with it . actually , i 've been here . in grad-school i bought crystal-meth from the maitre d ' .\n",
      "\n",
      "Decoding: that 's why it 's a classic . come on , man ... how 'bout hug ... ?\n",
      "Target:   that 's why it 's a classic . come on , man ... how 'bout a hug ... ?\n",
      "\n",
      "Decoding: how much is it ? a few thousand , at least . a rolex like that ... lucky for you 've missed it .\n",
      "Target:   how much is it ? a few thousand , at least . a rolex like that ... lucky for you they missed it .\n",
      "\n",
      "Decoding: i told you , they hired me over the phone . i 've never met anyone .\n",
      "Target:   i told you , they hired me over the phone . i never met anyone .\n",
      "\n",
      "Decoding: i do n't want money . i 'm pulling back curtain . i 'm here to meet the wizard .\n",
      "Target:   i do n't want money . i 'm pulling back the curtain . i 'm here to meet the wizard .\n",
      "\n",
      "Decoding: tell them the cops are after you ... tell them you got to talk to someone , i 'm threatening to blow the whistle .\n",
      "Target:   tell them the cops are after you ... tell them you 've got to talk to someone , i 'm threatening to blow the whistle .\n",
      "\n",
      "Decoding: they own the whole building . they just move from the floor to floor .\n",
      "Target:   they own the whole building . they just move from floor to floor .\n",
      "\n",
      "Decoding: look , it was just a job . nothing personal , ya know ? i play my part , improvise little . that 's what i 'm good at .\n",
      "Target:   look , it was just a job . nothing personal , ya know ? i play my part , improvise a little . that 's what i 'm good at .\n",
      "\n",
      "Decoding: that 's right -- you 're left-brain the word fetishist .\n",
      "Target:   that 's right -- you 're a left-brain word fetishist .\n",
      "\n",
      "Decoding: one guarantee . payment 's entirely at your brother discretion and , as a gift , dependent on your satisfaction .\n",
      "Target:   one guarantee . payment 's entirely at your brother 's discretion and , as a gift , dependent on your satisfaction .\n",
      "\n",
      "Decoding: your brother was a client with our branch . we do a sort of informal scoring . his numbers were outstanding . sure you 're not hungry at all ... ? tung hoy , best in chinatown ...\n",
      "Target:   your brother was a client with our london branch . we do a sort of informal scoring . his numbers were outstanding . sure you 're not hungry at all ... ? tung hoy , best in chinatown ...\n",
      "\n",
      "Decoding: key ?\n",
      "Target:   the key ?\n",
      "\n",
      "Decoding: nobody 's worried about your father .\n",
      "Target:   nobody worried about your father .\n",
      "\n",
      "Decoding: there 's been a break in . lock this door and stay here . do n't move muscle .\n",
      "Target:   there 's been a break in . lock this door and stay here . do n't move a muscle .\n",
      "\n",
      "Decoding: i do n't know what you 're talking about . what happened ?\n",
      "Target:   i do n't know what you 're talking about . what 's happened ?\n",
      "\n",
      "Decoding: did alarm go off ? the house ... they ... you did n't see ... ?\n",
      "Target:   did the alarm go off ? the house ... they ... you did n't see ... ?\n",
      "\n",
      "Decoding: then then .\n",
      "Target:   goodnight then .\n",
      "\n",
      "Decoding: okay . i think he into some sort of new personal improvement cult .\n",
      "Target:   okay . i think he 's into some sort of new personal improvement cult .\n",
      "\n",
      "Decoding: dinner in the oven .\n",
      "Target:   dinner 's in the oven .\n",
      "\n",
      "Decoding: there was incident a few days ago ... a nervous breakdown , they said . the police took him . they left this address , in case anyone ...\n",
      "Target:   there was an incident a few days ago ... a nervous breakdown , they said . the police took him . they left this address , in case anyone ...\n",
      "\n",
      "Decoding: what 's trouble ?\n",
      "Target:   what 's the trouble ?\n",
      "\n",
      "Decoding: mister ... seymour butts .\n",
      "Target:   a mister ... seymour butts .\n",
      "\n",
      "Decoding: what 's the gentleman , maria ?\n",
      "Target:   what gentleman , maria ?\n",
      "\n",
      "Decoding: i would n't mention following , except he was very insistent . it 's obviously some sort of prank ...\n",
      "Target:   i would n't mention the following , except he was very insistent . it 's obviously some sort of prank ...\n",
      "\n",
      "Decoding: i send your regrets . honestly , why must i even bother ?\n",
      "Target:   i 'll send your regrets . honestly , why must i even bother ?\n",
      "\n",
      "Decoding: the hinchberger 's wedding .\n",
      "Target:   the hinchberger wedding .\n",
      "\n",
      "Decoding: invitations : museum gala .\n",
      "Target:   invitations : the museum gala .\n",
      "\n",
      "Decoding: nice touch . does a game use real bullets ... ?\n",
      "Target:   nice touch . does the game use real bullets ... ?\n",
      "\n",
      "Decoding: it 's what they do . it 's like ... being toyed with by a bunch of ... depraved children\n",
      "Target:   it 's what they do . it 's like ... being toyed with by a bunch of ... depraved children .\n",
      "\n",
      "Decoding: find out about a company called the c .r .s . consumer recreation services .\n",
      "Target:   find out about a company called c .r .s . consumer recreation services .\n",
      "\n",
      "Decoding: someone 's playing hardball . it 's complicated . can i ask favor ?\n",
      "Target:   someone 's playing hardball . it 's complicated . can i ask a favor ?\n",
      "\n",
      "Decoding: how 's the concerned should i be ?\n",
      "Target:   how concerned should i be ?\n",
      "\n",
      "Decoding: that you 've a involved conrad ... is unforgivable . i am now your enemy .\n",
      "Target:   that you 've involved conrad ... is unforgivable . i am now your enemy .\n",
      "\n",
      "Decoding: what happened ...\n",
      "Target:   what 's happened ...\n",
      "\n",
      "Decoding: modelling small-group dynamics in formation of narrative hallucinations . you brought us here to scare us . insomnia , that was just a decoy issue . you 're disgusting .\n",
      "Target:   modelling small-group dynamics in the formation of narrative hallucinations . you brought us here to scare us . insomnia , that was just a decoy issue . you 're disgusting .\n",
      "\n",
      "Decoding: come on . these are the typically sentimental gestures of depraved industrialist .\n",
      "Target:   come on . these are the typically sentimental gestures of a depraved industrialist .\n",
      "\n",
      "Decoding: the children . children hugh crain built the house for . the children he never had .\n",
      "Target:   the children . the children hugh crain built the house for . the children he never had .\n",
      "\n",
      "Decoding: obsessive worrier . join club . and you ? i 'd guess ...\n",
      "Target:   obsessive worrier . join the club . and you ? i 'd guess ...\n",
      "\n",
      "Decoding: so why did you need the addam family mansion for a scientific test ?\n",
      "Target:   so why did you need the addam 's family mansion for a scientific test ?\n",
      "\n",
      "Decoding: -- how much is this car 's worth ?\n",
      "Target:   -- how much is this car worth ?\n",
      "\n",
      "Decoding: you do n't really believe it haunted ... do you believe in ghosts ?\n",
      "Target:   you do n't really believe it 's haunted ... do you believe in ghosts ?\n",
      "\n",
      "Decoding: so could you ! is this some fucked up the idea of art , putting someone else 's name to a painting ?\n",
      "Target:   so could you ! is this some fucked up idea of art , putting someone else 's name to a painting ?\n",
      "\n",
      "Decoding: and why did n't marrow tell < u > us < /u > ? does n't he a trust women ? that fuck .\n",
      "Target:   and why did n't marrow tell < u > us < /u > ? does n't he trust women ? that fuck .\n",
      "\n",
      "Decoding: nah , you 're going crazy with doubt , all of your mistakes are coming back up the pipes , and it 's worse than nightmare . --\n",
      "Target:   nah , you 're going crazy with doubt , all of your mistakes are coming back up the pipes , and it 's worse than a nightmare . --\n",
      "\n",
      "Decoding: not the way you 've constructed your group , it just not ethical !\n",
      "Target:   not the way you 've constructed your group , it 's just not ethical !\n",
      "\n",
      "Decoding: children want me . they 're calling me . they need me .\n",
      "Target:   the children want me . they 're calling me . they need me .\n",
      "\n",
      "Decoding: i looked at theo . she had look on her face .\n",
      "Target:   i looked at theo . she had a look on her face .\n",
      "\n",
      "Decoding: i was n't thinking about my mother bathroom .\n",
      "Target:   i was n't thinking about my mother 's bathroom .\n",
      "\n",
      "Decoding: so ... smell ... is ... smell is sense that triggers the most powerful memories . and memory can trigger a smell .\n",
      "Target:   so ... smell ... is ... smell is the sense that triggers the most powerful memories . and a memory can trigger a smell .\n",
      "\n",
      "Decoding: in the bathroom in my mother 's room , toilet was next to old wooden table . it smelled like that wood .\n",
      "Target:   in the bathroom in my mother 's room , the toilet was next to an old wooden table . it smelled like that wood .\n",
      "\n",
      "Decoding: cold sensation . who felt it first ?\n",
      "Target:   the cold sensation . who felt it first ?\n",
      "\n",
      "Decoding: i really ... honored to be part of this study , jim .\n",
      "Target:   i 'm really ... honored to be part of this study , jim .\n",
      "\n",
      "Decoding: nell . good enough . and i jim .\n",
      "Target:   nell . good enough . and i 'm jim .\n",
      "\n",
      "Decoding: that ? that 's a hill house .\n",
      "Target:   that ? that 's hill house .\n",
      "\n",
      "Decoding: here 's how they 're organized . groups of five , very different personalities : scored all over the kiersey temperament sorter just like you asked for . and they all score high on insomnia charts .\n",
      "Target:   here 's how they 're organized . groups of five , very different personalities : scored all over the kiersey temperament sorter just like you asked for . and they all score high on the insomnia charts .\n",
      "\n",
      "Decoding: you hear the vibrations in the wire . there 's magnetic pulse in the wires , you feel it . i could test it .\n",
      "Target:   you hear the vibrations in the wire . there 's a magnetic pulse in the wires , you feel it . i could test it .\n",
      "\n",
      "Decoding: but experiment was a failure .\n",
      "Target:   but the experiment was a failure .\n",
      "\n",
      "Decoding: he wandering around house , and nell heard him . she thought it was ghosts . let 's go look for him again .\n",
      "Target:   he 's wandering around the house , and nell heard him . she thought it was ghosts . let 's go look for him again .\n",
      "\n",
      "Decoding: i 'll take her with me to university tomorrow . i ca n't believe i read the test wrong . i did n't see anything that looked like she was suicidal .\n",
      "Target:   i 'll take her with me to the university tomorrow . i ca n't believe i read the test wrong . i did n't see anything that looked like she was suicidal .\n",
      "\n",
      "Decoding: no , but nell been here longer than i have .\n",
      "Target:   no , but nell 's been here longer than i have .\n",
      "\n",
      "Decoding: rene crain . up there . rope . ship 's hawser . hard to tie . do n't know how she 's got it .\n",
      "Target:   rene crain . up there . rope . ship 's hawser . hard to tie . do n't know how she got it .\n",
      "\n",
      "Decoding: mrs . dudley be waiting for you .\n",
      "Target:   mrs . dudley 'll be waiting for you .\n",
      "\n",
      "Decoding: that 's a good question . what is it about fences ? sometimes a locked chain makes people on both sides of fence just a little more comfortable . why would that be ?\n",
      "Target:   that 's a good question . what is it about fences ? sometimes a locked chain makes people on both sides of the fence just a little more comfortable . why would that be ?\n",
      "\n",
      "Decoding: well , i 've never lived with a beauty . you must love working here .\n",
      "Target:   well , i 've never lived with beauty . you must love working here .\n",
      "\n",
      "Decoding: nell , it makes sense . it 's all makes sense . you and i , we were scaring each other , working each other up .\n",
      "Target:   nell , it makes sense . it all makes sense . you and i , we were scaring each other , working each other up .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for decoding, target in errors:\n",
    "    print(\"Decoding: \" + \" \".join(decoding))\n",
    "    print(\"Target:   \" + \" \".join(target) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
