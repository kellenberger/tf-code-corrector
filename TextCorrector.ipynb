{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import nltk\n",
    "\n",
    "from correct_text import train, decode, decode_sentence, evaluate_accuracy, create_model,\\\n",
    "    get_corrective_tokens, DefaultPTBConfig, DefaultMovieDialogConfig\n",
    "from text_corrector_data_readers import PTBDataReader, MovieDialogReader\n",
    "\n",
    "from utils.standard_hparams_utils import create_standard_hparams\n",
    "from train import train\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_path = \"C:\\\\Users\\\\svenk\\\\Documents\\\\dialog_corpus\"\n",
    "# root_data_path = \"E:\\\\Documents\\\\dialog_corpus\"\n",
    "full_path = os.path.join(root_data_path, \"movie_lines.txt\")\n",
    "train_path = os.path.join(root_data_path, \"original_train.txt\")\n",
    "perturbed_train_path = os.path.join(root_data_path, \"perturbed_train.txt\")\n",
    "val_path = os.path.join(root_data_path, \"val.txt\")\n",
    "perturbed_val_path = os.path.join(root_data_path, \"perturbed_val.txt\")\n",
    "test_path = os.path.join(root_data_path, \"test.txt\")\n",
    "perturbed_test_path = os.path.join(root_data_path, \"perturbed_test.txt\")\n",
    "model_path = os.path.join(root_data_path, \"dialog_correcter_model_testnltk\")\n",
    "config = DefaultMovieDialogConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Lines: 213299/304713, 69.99996718223377%\n",
      "Val Lines: 45707/304713, 15.000016408883113%\n",
      "Test Lines: 45707/304713, 15.000016408883113%\n"
     ]
    }
   ],
   "source": [
    "DROPOUT_TOKENS = {\"a\", \"an\", \"the\", \"'ll\", \"'s\", \"'m\", \"'ve\"}\n",
    "\n",
    "REPLACEMENTS = {\"there\": \"their\", \"their\": \"there\", \"then\": \"than\", \"than\": \"then\"}\n",
    "\n",
    "count = 0\n",
    "with open(full_path, \"r\") as raw_data:\n",
    "        for line in raw_data:\n",
    "            count += 1\n",
    "train_count = 0\n",
    "val_count = 0\n",
    "test_count = 0\n",
    "with open(full_path, \"r\") as raw_data, \\\n",
    "            open(train_path, \"w\") as train, \\\n",
    "            open(perturbed_train_path, \"w\") as perturbed_train, \\\n",
    "            open(val_path, \"w\") as val, \\\n",
    "            open(perturbed_val_path, \"w\") as perturbed_val, \\\n",
    "            open(test_path, \"w\") as test, \\\n",
    "            open(perturbed_test_path, \"w\") as perturbed_test:\n",
    "        for i, line in enumerate(raw_data):\n",
    "            tokens = line.lower().strip().split()\n",
    "            source = []\n",
    "            target = []\n",
    "\n",
    "            for token in tokens:\n",
    "                target.append(token)\n",
    "\n",
    "                # Randomly dropout some words from the input.\n",
    "                dropout_token = (token in DROPOUT_TOKENS and\n",
    "                                random.random() < 0.25)\n",
    "                replace_token = (token in REPLACEMENTS and\n",
    "                                random.random() < 0.25)\n",
    "\n",
    "                if replace_token:\n",
    "                    source.append(REPLACEMENTS[token])\n",
    "                elif not dropout_token:\n",
    "                    source.append(token)\n",
    "                    \n",
    "            if i+1 < 0.7*count:\n",
    "                train.write(\" \".join(target) + \"\\n\")\n",
    "                perturbed_train.write(\" \".join(source) + \"\\n\")\n",
    "                train_count += 1\n",
    "            elif i+1 < 0.85*count:\n",
    "                val.write(\" \".join(target) + \"\\n\")\n",
    "                perturbed_val.write(\" \".join(source) + \"\\n\")\n",
    "                val_count += 1\n",
    "            else:\n",
    "                test.write(\" \".join(target) + \"\\n\")\n",
    "                perturbed_test.write(\" \".join(source) + \"\\n\")\n",
    "                test_count += 1\n",
    "print(\"Train Lines: {}/{}, {}%\".format(train_count, count, train_count * 100.0 / count))\n",
    "print(\"Val Lines: {}/{}, {}%\".format(val_count, count, val_count * 100.0 / count))\n",
    "print(\"Test Lines: {}/{}, {}%\".format(test_count, count, test_count * 100.0 / count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = create_standard_hparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reader = MovieDialogReader(config, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data; train = C:\\Users\\svenk\\Documents\\dialog_corpus\\train.txt, test = C:\\Users\\svenk\\Documents\\dialog_corpus\\val.txt\n",
      "Creating 4 layers of 512 units.\n",
      "Reading model parameters from C:\\Users\\svenk\\Documents\\dialog_corpus\\dialog_correcter_model_testnltk\\translate.ckpt-10800\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\svenk\\Documents\\dialog_corpus\\dialog_correcter_model_testnltk\\translate.ckpt-10800\n",
      "Training bucket sizes: [198232, 85878, 50002, 70314]\n",
      "Total train size: 404426.0\n",
      "global step 10900 learning rate 0.3850 step-time 6.70 perplexity 21.27\n",
      "  eval: bucket 0 perplexity 3.61\n",
      "  eval: bucket 1 perplexity 40.10\n",
      "  eval: bucket 2 perplexity 120.03\n",
      "  eval: bucket 3 perplexity 175.38\n",
      "global step 11000 learning rate 0.3850 step-time 5.66 perplexity 21.15\n",
      "  eval: bucket 0 perplexity 7.59\n",
      "  eval: bucket 1 perplexity 71.84\n",
      "  eval: bucket 2 perplexity 172.14\n",
      "  eval: bucket 3 perplexity 361.24\n",
      "global step 11100 learning rate 0.3850 step-time 5.85 perplexity 29.12\n",
      "  eval: bucket 0 perplexity 4.33\n",
      "  eval: bucket 1 perplexity 45.23\n",
      "  eval: bucket 2 perplexity 126.60\n",
      "  eval: bucket 3 perplexity 304.59\n",
      "global step 11200 learning rate 0.3850 step-time 6.09 perplexity 20.04\n",
      "  eval: bucket 0 perplexity 3.48\n",
      "  eval: bucket 1 perplexity 29.61\n",
      "  eval: bucket 2 perplexity 57.16\n",
      "  eval: bucket 3 perplexity 120.22\n",
      "global step 11300 learning rate 0.3850 step-time 6.72 perplexity 19.86\n",
      "  eval: bucket 0 perplexity 3.73\n",
      "  eval: bucket 1 perplexity 24.58\n",
      "  eval: bucket 2 perplexity 43.68\n",
      "  eval: bucket 3 perplexity 54.12\n",
      "global step 11400 learning rate 0.3850 step-time 5.34 perplexity 8.14\n",
      "  eval: bucket 0 perplexity 2.62\n",
      "  eval: bucket 1 perplexity 16.19\n",
      "  eval: bucket 2 perplexity 26.28\n",
      "  eval: bucket 3 perplexity 46.13\n",
      "global step 11500 learning rate 0.3850 step-time 6.00 perplexity 7.34\n",
      "  eval: bucket 0 perplexity 2.40\n",
      "  eval: bucket 1 perplexity 9.69\n",
      "  eval: bucket 2 perplexity 15.34\n",
      "  eval: bucket 3 perplexity 40.36\n",
      "global step 11600 learning rate 0.3850 step-time 6.24 perplexity 6.43\n",
      "  eval: bucket 0 perplexity 2.44\n",
      "  eval: bucket 1 perplexity 9.32\n",
      "  eval: bucket 2 perplexity 18.95\n",
      "  eval: bucket 3 perplexity 36.34\n",
      "global step 11700 learning rate 0.3850 step-time 6.13 perplexity 8.40\n",
      "  eval: bucket 0 perplexity 2.89\n",
      "  eval: bucket 1 perplexity 33.45\n",
      "  eval: bucket 2 perplexity 87.12\n",
      "  eval: bucket 3 perplexity 108.60\n",
      "global step 11800 learning rate 0.3812 step-time 6.64 perplexity 21.24\n",
      "  eval: bucket 0 perplexity 3.06\n",
      "  eval: bucket 1 perplexity 39.98\n",
      "  eval: bucket 2 perplexity 152.25\n",
      "  eval: bucket 3 perplexity 731.00\n",
      "global step 11900 learning rate 0.3774 step-time 5.50 perplexity 47.08\n",
      "  eval: bucket 0 perplexity 9.70\n",
      "  eval: bucket 1 perplexity 348.99\n",
      "  eval: bucket 2 perplexity 3023.10\n",
      "  eval: bucket 3 perplexity 17174.54\n",
      "global step 12000 learning rate 0.3736 step-time 6.08 perplexity 42.45\n",
      "  eval: bucket 0 perplexity 3.47\n",
      "  eval: bucket 1 perplexity 33.01\n",
      "  eval: bucket 2 perplexity 120.12\n",
      "  eval: bucket 3 perplexity 735.40\n",
      "global step 12100 learning rate 0.3736 step-time 5.89 perplexity 39.09\n",
      "  eval: bucket 0 perplexity 2.60\n",
      "  eval: bucket 1 perplexity 74.94\n",
      "  eval: bucket 2 perplexity 443.05\n",
      "  eval: bucket 3 perplexity 9173.13\n",
      "global step 12200 learning rate 0.3736 step-time 5.48 perplexity 28.06\n",
      "  eval: bucket 0 perplexity 3.35\n",
      "  eval: bucket 1 perplexity 53.92\n",
      "  eval: bucket 2 perplexity 267.81\n",
      "  eval: bucket 3 perplexity 797.93\n",
      "global step 12300 learning rate 0.3736 step-time 6.38 perplexity 61.32\n",
      "  eval: bucket 0 perplexity 5.36\n",
      "  eval: bucket 1 perplexity 43.80\n",
      "  eval: bucket 2 perplexity 333.50\n",
      "  eval: bucket 3 perplexity 1626.62\n",
      "global step 12400 learning rate 0.3699 step-time 5.85 perplexity 31.75\n",
      "  eval: bucket 0 perplexity 2.73\n",
      "  eval: bucket 1 perplexity 52.87\n",
      "  eval: bucket 2 perplexity 238.54\n",
      "  eval: bucket 3 perplexity 1275.90\n",
      "global step 12500 learning rate 0.3699 step-time 5.80 perplexity 19.80\n",
      "  eval: bucket 0 perplexity 2.57\n",
      "  eval: bucket 1 perplexity 39.11\n",
      "  eval: bucket 2 perplexity 151.77\n",
      "  eval: bucket 3 perplexity 362.67\n",
      "global step 12600 learning rate 0.3699 step-time 6.58 perplexity 25.76\n",
      "  eval: bucket 0 perplexity 2.88\n",
      "  eval: bucket 1 perplexity 73.90\n",
      "  eval: bucket 2 perplexity 232.76\n",
      "  eval: bucket 3 perplexity 729.92\n",
      "global step 12700 learning rate 0.3699 step-time 6.29 perplexity 27.68\n",
      "  eval: bucket 0 perplexity 3.82\n",
      "  eval: bucket 1 perplexity 41.95\n",
      "  eval: bucket 2 perplexity 140.44\n",
      "  eval: bucket 3 perplexity 392.91\n",
      "global step 12800 learning rate 0.3699 step-time 5.31 perplexity 18.94\n",
      "  eval: bucket 0 perplexity 5.26\n",
      "  eval: bucket 1 perplexity 68.00\n",
      "  eval: bucket 2 perplexity 189.67\n",
      "  eval: bucket 3 perplexity 525.96\n",
      "global step 12900 learning rate 0.3699 step-time 6.24 perplexity 28.09\n",
      "  eval: bucket 0 perplexity 5.31\n",
      "  eval: bucket 1 perplexity 76.46\n",
      "  eval: bucket 2 perplexity 342.89\n",
      "  eval: bucket 3 perplexity 943.03\n",
      "global step 13000 learning rate 0.3662 step-time 5.23 perplexity 34.54\n",
      "  eval: bucket 0 perplexity 5.51\n",
      "  eval: bucket 1 perplexity 74.28\n",
      "  eval: bucket 2 perplexity 217.17\n",
      "  eval: bucket 3 perplexity 563.75\n",
      "global step 13100 learning rate 0.3625 step-time 5.86 perplexity 27.83\n",
      "  eval: bucket 0 perplexity 5.51\n",
      "  eval: bucket 1 perplexity 59.51\n",
      "  eval: bucket 2 perplexity 182.15\n",
      "  eval: bucket 3 perplexity 780.95\n",
      "global step 13200 learning rate 0.3625 step-time 5.10 perplexity 15.35\n",
      "  eval: bucket 0 perplexity 4.37\n",
      "  eval: bucket 1 perplexity 33.51\n",
      "  eval: bucket 2 perplexity 151.38\n",
      "  eval: bucket 3 perplexity 304.97\n",
      "global step 13300 learning rate 0.3625 step-time 6.10 perplexity 20.32\n",
      "  eval: bucket 0 perplexity 3.48\n",
      "  eval: bucket 1 perplexity 30.98\n",
      "  eval: bucket 2 perplexity 71.65\n",
      "  eval: bucket 3 perplexity 227.54\n",
      "global step 13400 learning rate 0.3625 step-time 6.14 perplexity 21.64\n",
      "  eval: bucket 0 perplexity 4.02\n",
      "  eval: bucket 1 perplexity 41.18\n",
      "  eval: bucket 2 perplexity 108.92\n",
      "  eval: bucket 3 perplexity 409.99\n",
      "global step 13500 learning rate 0.3625 step-time 6.01 perplexity 30.02\n",
      "  eval: bucket 0 perplexity 4.56\n",
      "  eval: bucket 1 perplexity 47.80\n",
      "  eval: bucket 2 perplexity 158.42\n",
      "  eval: bucket 3 perplexity 508.35\n",
      "global step 13600 learning rate 0.3589 step-time 6.03 perplexity 32.28\n",
      "  eval: bucket 0 perplexity 6.53\n",
      "  eval: bucket 1 perplexity 46.84\n",
      "  eval: bucket 2 perplexity 121.52\n",
      "  eval: bucket 3 perplexity 238.99\n",
      "global step 13700 learning rate 0.3553 step-time 5.60 perplexity 25.29\n",
      "  eval: bucket 0 perplexity 9.77\n",
      "  eval: bucket 1 perplexity 35.66\n",
      "  eval: bucket 2 perplexity 65.10\n",
      "  eval: bucket 3 perplexity 91.45\n",
      "global step 13800 learning rate 0.3553 step-time 5.89 perplexity 17.59\n",
      "  eval: bucket 0 perplexity 3.96\n",
      "  eval: bucket 1 perplexity 30.96\n",
      "  eval: bucket 2 perplexity 60.59\n",
      "  eval: bucket 3 perplexity 95.63\n",
      "global step 13900 learning rate 0.3553 step-time 4.91 perplexity 12.64\n",
      "  eval: bucket 0 perplexity 6.07\n",
      "  eval: bucket 1 perplexity 33.65\n",
      "  eval: bucket 2 perplexity 54.98\n",
      "  eval: bucket 3 perplexity 91.18\n",
      "global step 14000 learning rate 0.3553 step-time 6.13 perplexity 21.50\n",
      "  eval: bucket 0 perplexity 5.24\n",
      "  eval: bucket 1 perplexity 41.95\n",
      "  eval: bucket 2 perplexity 75.09\n",
      "  eval: bucket 3 perplexity 131.49\n",
      "global step 14100 learning rate 0.3553 step-time 5.57 perplexity 20.40\n",
      "  eval: bucket 0 perplexity 4.83\n",
      "  eval: bucket 1 perplexity 37.43\n",
      "  eval: bucket 2 perplexity 81.29\n",
      "  eval: bucket 3 perplexity 139.56\n",
      "global step 14200 learning rate 0.3553 step-time 6.33 perplexity 16.89\n",
      "  eval: bucket 0 perplexity 3.93\n",
      "  eval: bucket 1 perplexity 37.84\n",
      "  eval: bucket 2 perplexity 68.49\n",
      "  eval: bucket 3 perplexity 90.94\n",
      "global step 14300 learning rate 0.3553 step-time 6.31 perplexity 21.56\n",
      "  eval: bucket 0 perplexity 5.41\n",
      "  eval: bucket 1 perplexity 41.56\n",
      "  eval: bucket 2 perplexity 131.66\n",
      "  eval: bucket 3 perplexity 234.86\n",
      "global step 14400 learning rate 0.3517 step-time 6.09 perplexity 18.87\n",
      "  eval: bucket 0 perplexity 3.56\n",
      "  eval: bucket 1 perplexity 34.58\n",
      "  eval: bucket 2 perplexity 76.72\n",
      "  eval: bucket 3 perplexity 156.07\n",
      "global step 14500 learning rate 0.3517 step-time 5.97 perplexity 13.69\n",
      "  eval: bucket 0 perplexity 4.12\n",
      "  eval: bucket 1 perplexity 30.03\n",
      "  eval: bucket 2 perplexity 105.04\n",
      "  eval: bucket 3 perplexity 284.10\n",
      "global step 14600 learning rate 0.3517 step-time 6.62 perplexity 18.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  eval: bucket 0 perplexity 2.77\n",
      "  eval: bucket 1 perplexity 21.29\n",
      "  eval: bucket 2 perplexity 70.76\n",
      "  eval: bucket 3 perplexity 139.60\n",
      "global step 14700 learning rate 0.3517 step-time 5.98 perplexity 9.24\n",
      "  eval: bucket 0 perplexity 2.55\n",
      "  eval: bucket 1 perplexity 25.23\n",
      "  eval: bucket 2 perplexity 43.40\n",
      "  eval: bucket 3 perplexity 81.04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-6d7a3268a1b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_reader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\deep-code-corrector\\correct_text.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(data_reader, train_path, test_path, model_path)\u001b[0m\n\u001b[0;32m    180\u001b[0m                 train_data, bucket_id)\n\u001b[0;32m    181\u001b[0m             _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n\u001b[1;32m--> 182\u001b[1;33m                                          target_weights, bucket_id, False)\n\u001b[0m\u001b[0;32m    183\u001b[0m             \u001b[0mstep_time\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                 \u001b[1;33m.\u001b[0m\u001b[0msteps_per_checkpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\deep-code-corrector\\text_corrector_models.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, session, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only, corrective_tokens)\u001b[0m\n\u001b[0;32m    286\u001b[0m                 \u001b[0moutput_feed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_feed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mforward_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[1;31m# Gradient norm, loss, no outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train(data_reader, train_path, val_path, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reader = MovieDialogReader(config, train_path, dropout_prob=0.25, replacement_prob=0.25, dataset_copies=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrective_tokens = get_corrective_tokens(data_reader, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(root_data_path, \"corrective_tokens.pickle\"), \"wb\") as f:\n",
    "    pickle.dump(corrective_tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(root_data_path, \"token_to_id.pickle\"), \"wb\") as f:\n",
    "    pickle.dump(data_reader.token_to_id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from C:\\Users\\svenk\\Documents\\dialog_corpus\\dialog_correcter_model_testnltk\\translate.ckpt-14700\n",
      "INFO:tensorflow:Restoring parameters from C:\\Users\\svenk\\Documents\\dialog_corpus\\dialog_correcter_model_testnltk\\translate.ckpt-14700\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = create_model(sess, True, model_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: you must have girlfriend\n",
      "Output: you must an an\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test a sample from the test dataset.\n",
    "decoded = decode_sentence(sess, model, data_reader, \"you must have girlfriend\", corrective_tokens=corrective_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you', 'must', 'have', 'the', 'must', 'have']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: did n't you say that they 're going to develop this revolutionary new thing ...\n",
      "Output: did you 're to develop to revolutionary ... you 're to UNK ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded = decode_sentence(sess, model, data_reader,\n",
    "                          \"did n't you say that they 're going to develop this revolutionary new thing ...\",\n",
    "                          corrective_tokens=corrective_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kvothe', 'went', 'to', 'UNK']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(sess, model, data_reader, \"kvothe went to market\", corrective_tokens=corrective_tokens, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['blablahblah', 'and', 'and', 'bladdddd', 'to', 'UNK']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(sess, model, data_reader, \"blablahblah and bladdddd went to market\", corrective_tokens=corrective_tokens,\n",
    "                verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do', 'you', 'have']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(sess, model, data_reader, \"do you have book\", corrective_tokens=corrective_tokens, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'cardinals', 'did', 'cubs', 'UNK', 'the', 'the', 'UNK', 'UNK']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(sess, model, data_reader, \"the cardinals did better then the cubs\", corrective_tokens=corrective_tokens, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-3ac453e69476>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 4 layers, 40k steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_reader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrective_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, max_samples=1000)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\deep-code-corrector\\correct_text.py\u001b[0m in \u001b[0;36mevaluate_accuracy\u001b[1;34m(sess, model, data_reader, corrective_tokens, test_path, max_samples)\u001b[0m\n\u001b[0;32m    354\u001b[0m         decoding = next(\n\u001b[0;32m    355\u001b[0m             decode(sess, model, data_reader, [source],\n\u001b[1;32m--> 356\u001b[1;33m                    corrective_tokens=corrective_tokens, verbose=False))\n\u001b[0m\u001b[0;32m    357\u001b[0m         \u001b[0mmodel_hypotheses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdecoding\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\deep-code-corrector\\correct_text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(sess, model, data_reader, data_to_decode, corrective_tokens, verbose)\u001b[0m\n\u001b[0;32m    274\u001b[0m         _, _, output_logits = model.step(\n\u001b[0;32m    275\u001b[0m             \u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbucket_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m             True, corrective_tokens=corrective_tokens_mask)\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         oov_input_tokens = [token for token in tokens if\n",
      "\u001b[1;32m~\\deep-code-corrector\\text_corrector_models.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, session, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only, corrective_tokens)\u001b[0m\n\u001b[0;32m    286\u001b[0m                 \u001b[0moutput_feed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_feed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mforward_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[1;31m# Gradient norm, loss, no outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4 layers, 40k steps\n",
    "errors = evaluate_accuracy(sess, model, data_reader, corrective_tokens, test_path)#, max_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 0: (10, 10)\n",
      "\tBaseline BLEU = 0.8368\n",
      "\tModel BLEU = 0.8425\n",
      "\tBaseline Accuracy: 0.9110\n",
      "\tModel Accuracy: 0.9303\n",
      "Bucket 1: (15, 15)\n",
      "\tBaseline BLEU = 0.8818\n",
      "\tModel BLEU = 0.8459\n",
      "\tBaseline Accuracy: 0.8063\n",
      "\tModel Accuracy: 0.8014\n",
      "Bucket 2: (20, 20)\n",
      "\tBaseline BLEU = 0.8891\n",
      "\tModel BLEU = 0.7986\n",
      "\tBaseline Accuracy: 0.7309\n",
      "\tModel Accuracy: 0.6281\n",
      "Bucket 3: (40, 40)\n",
      "\tBaseline BLEU = 0.9099\n",
      "\tModel BLEU = 0.5997\n",
      "\tBaseline Accuracy: 0.6007\n",
      "\tModel Accuracy: 0.1607\n"
     ]
    }
   ],
   "source": [
    "# 4 layers, 30k steps\n",
    "errors = evaluate_accuracy(sess, model, data_reader, corrective_tokens, test_path)#, max_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 0: (10, 10)\n",
      "\tBaseline BLEU = 0.8330\n",
      "\tModel BLEU = 0.8335\n",
      "\tBaseline Accuracy: 0.9067\n",
      "\tModel Accuracy: 0.9218\n",
      "Bucket 1: (15, 15)\n",
      "\tBaseline BLEU = 0.8772\n",
      "\tModel BLEU = 0.8100\n",
      "\tBaseline Accuracy: 0.7980\n",
      "\tModel Accuracy: 0.7437\n",
      "Bucket 2: (20, 20)\n",
      "\tBaseline BLEU = 0.8898\n",
      "\tModel BLEU = 0.7636\n",
      "\tBaseline Accuracy: 0.7366\n",
      "\tModel Accuracy: 0.5370\n",
      "Bucket 3: (40, 40)\n",
      "\tBaseline BLEU = 0.9098\n",
      "\tModel BLEU = 0.5387\n",
      "\tBaseline Accuracy: 0.6041\n",
      "\tModel Accuracy: 0.1117\n"
     ]
    }
   ],
   "source": [
    "# 4 layers, 20k steps\n",
    "errors = evaluate_accuracy(sess, model, data_reader, corrective_tokens, test_path)#, max_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 0: (10, 10)\n",
      "\tBaseline BLEU = 0.8341\n",
      "\tModel BLEU = 0.8516\n",
      "\tBaseline Accuracy: 0.9083\n",
      "\tModel Accuracy: 0.9384\n",
      "Bucket 1: (15, 15)\n",
      "\tBaseline BLEU = 0.8850\n",
      "\tModel BLEU = 0.8860\n",
      "\tBaseline Accuracy: 0.8156\n",
      "\tModel Accuracy: 0.8491\n",
      "Bucket 2: (20, 20)\n",
      "\tBaseline BLEU = 0.8876\n",
      "\tModel BLEU = 0.8880\n",
      "\tBaseline Accuracy: 0.7291\n",
      "\tModel Accuracy: 0.7817\n",
      "Bucket 3: (40, 40)\n",
      "\tBaseline BLEU = 0.9099\n",
      "\tModel BLEU = 0.9045\n",
      "\tBaseline Accuracy: 0.6073\n",
      "\tModel Accuracy: 0.6425\n"
     ]
    }
   ],
   "source": [
    "errors = evaluate_accuracy(sess, model, data_reader, corrective_tokens, test_path)#, max_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding: you beg for mercy in a second .\n",
      "Target:   you 'll beg for mercy in a second .\n",
      "\n",
      "Decoding: i 'm dying for a shower . you could use the one too . and we 'd better check that bandage .\n",
      "Target:   i 'm dying for a shower . you could use one too . and we 'd better check that bandage .\n",
      "\n",
      "Decoding: whatever ... they 've become hotshot computer guys so they get a job to build el computer grande ... skynet ... for the government . right ?\n",
      "Target:   whatever ... they become the hotshot computer guys so they get the job to build el computer grande ... skynet ... for the government . right ?\n",
      "\n",
      "Decoding: did n't you say that they 're going to develop this revolutionary a new thing ...\n",
      "Target:   did n't you say that they 're going to develop this revolutionary new thing ...\n",
      "\n",
      "Decoding: bag some z ?\n",
      "Target:   bag some z 's ?\n",
      "\n",
      "Decoding: sleep . it 'll be a light soon .\n",
      "Target:   sleep . it 'll be light soon .\n",
      "\n",
      "Decoding: well , at least i know what to name him . i do n't suppose you 'd know who father is ? so i do n't tell him to get lost when i meet him .\n",
      "Target:   well , at least i know what to name him . i do n't suppose you 'd know who the father is ? so i do n't tell him to get lost when i meet him .\n",
      "\n",
      "Decoding: we got ta get you to doctor .\n",
      "Target:   we got ta get you to a doctor .\n",
      "\n",
      "Decoding: hunter killers . patrol machines . a build in automated factories . most of us were rounded up , put in camps ... for orderly disposal .\n",
      "Target:   hunter killers . patrol machines . build in automated factories . most of us were rounded up , put in camps ... for orderly disposal .\n",
      "\n",
      "Decoding: but outside , it 's a living human tissue . flesh , skin , hair ... blood . grown for the cyborgs .\n",
      "Target:   but outside , it 's living human tissue . flesh , skin , hair ... blood . grown for the cyborgs .\n",
      "\n",
      "Decoding: you heard enough . decide . are you going to release me ?\n",
      "Target:   you 've heard enough . decide . are you going to release me ?\n",
      "\n",
      "Decoding: okay . okay . but this ... cyborg ... if it metal ...\n",
      "Target:   okay . okay . but this ... cyborg ... if it 's metal ...\n",
      "\n",
      "Decoding: you go naked . something about the field generated by living organism . nothing dead will go .\n",
      "Target:   you go naked . something about the field generated by a living organism . nothing dead will go .\n",
      "\n",
      "Decoding: ca n't . nobody goes home . nobody else comes through . it just him and me .\n",
      "Target:   ca n't . nobody goes home . nobody else comes through . it 's just him and me .\n",
      "\n",
      "Decoding: i see . and this ... computer , thinks it can win by killing the mother of its enemy , kill- ing him , in effect , before he is even conceived ? sort of retroactive abortion ?\n",
      "Target:   i see . and this ... computer , thinks it can win by killing the mother of its enemy , kill- ing him , in effect , before he is even conceived ? a sort of retroactive abortion ?\n",
      "\n",
      "Decoding: skynet . a computer defense system built for sac-norad by cyber dynamics . modified series 4800 .\n",
      "Target:   skynet . a computer defense system built for sac-norad by cyber dynamics . a modified series 4800 .\n",
      "\n",
      "Decoding: a year 2027 ?\n",
      "Target:   the year 2027 ?\n",
      "\n",
      "Decoding: with one thirty a second under perry , from '21 to '27 --\n",
      "Target:   with the one thirty second under perry , from '21 to '27 --\n",
      "\n",
      "Decoding: why do n't you just stretch out here and get some sleep . it take your mom 's a good hour to get here from redlands .\n",
      "Target:   why do n't you just stretch out here and get some sleep . it 'll take your mom a good hour to get here from redlands .\n",
      "\n",
      "Decoding: lieutenant , are you sure it them ? maybe i should see the ... bodies .\n",
      "Target:   lieutenant , are you sure it 's them ? maybe i should see the ... bodies .\n",
      "\n",
      "Decoding: i already did . no answer at the door and the apartment manager 's out . i keeping them there .\n",
      "Target:   i already did . no answer at the door and the apartment manager 's out . i 'm keeping them there .\n",
      "\n",
      "Decoding: that stuff two hours cold .\n",
      "Target:   that stuff 's two hours cold .\n",
      "\n",
      "Decoding: you got ta be kidding me . the new guys 'll be short-stroking it over this one . one-day pattern killer .\n",
      "Target:   you got ta be kidding me . the new guys 'll be short-stroking it over this one . a one-day pattern killer .\n",
      "\n",
      "Decoding: give me a short version .\n",
      "Target:   give me the short version .\n",
      "\n",
      "Decoding: because it 's fair . give me the next quarter . if you still feel this way , vote your shares ...\n",
      "Target:   because it 's fair . give me next quarter . if you still feel this way , vote your shares ...\n",
      "\n",
      "Decoding: it 's probably will . in fact , i 'd go so far as to say it 's almost certainly will , in time . why should i settle for that ?\n",
      "Target:   it probably will . in fact , i 'd go so far as to say it almost certainly will , in time . why should i settle for that ?\n",
      "\n",
      "Decoding: stock will turn .\n",
      "Target:   the stock will turn .\n",
      "\n",
      "Decoding: you want to know what it is ? what 's it all about ? john . chapter nine . verse twenty-five .\n",
      "Target:   you want to know what it is ? what it 's all about ? john . chapter nine . verse twenty-five .\n",
      "\n",
      "Decoding: i only mention it because i took a test this afternoon , down on montgomery street .\n",
      "Target:   i only mention it because i took the test this afternoon , down on montgomery street .\n",
      "\n",
      "Decoding: christine ! mister van orton is valued customer ...\n",
      "Target:   christine ! mister van orton is a valued customer ...\n",
      "\n",
      "Decoding: a single ?\n",
      "Target:   single ?\n",
      "\n",
      "Decoding: there 's another gig starting in saudi arabia . i just a walk-on this time though . bit-part .\n",
      "Target:   there 's another gig starting in saudi arabia . i 'm just a walk-on this time though . bit-part .\n",
      "\n",
      "Decoding: no ! you take another step , i shoot ! they 're trying to kill me ...\n",
      "Target:   no ! you take another step , i 'll shoot ! they 're trying to kill me ...\n",
      "\n",
      "Decoding: listen very carefully , i 'm telling the truth ... this is a game . this was all the game .\n",
      "Target:   listen very carefully , i 'm telling the truth ... this is the game . this was all the game .\n",
      "\n",
      "Decoding: that 's gun . that 's ... that 's not automatic . the guard had an automatic ...\n",
      "Target:   that gun . that ... that 's not automatic . the guard had an automatic ...\n",
      "\n",
      "Decoding: take a picture out .\n",
      "Target:   take the picture out .\n",
      "\n",
      "Decoding: yeah . first communion . are n't i little angel ?\n",
      "Target:   yeah . first communion . are n't i a little angel ?\n",
      "\n",
      "Decoding: let me go get some clothes on . we talk , okay ? be right back .\n",
      "Target:   let me go get some clothes on . we 'll talk , okay ? be right back .\n",
      "\n",
      "Decoding: i 'm tired . i 'm sorry , i should go . i 've been enough of nuisance .\n",
      "Target:   i 'm tired . i 'm sorry , i should go . i 've been enough of a nuisance .\n",
      "\n",
      "Decoding: they said five hundred . i said six . they said man in the gray flannel suit . i think i said , you mean the attractive guy in the gray flannel suit ?\n",
      "Target:   they said five hundred . i said six . they said the man in the gray flannel suit . i think i said , you mean the attractive guy in the gray flannel suit ?\n",
      "\n",
      "Decoding: i have a confession to make . someone gave me six-hundred dollars to spill a drinks on you , as a practical joke .\n",
      "Target:   i have a confession to make . someone gave me six-hundred dollars to spill drinks on you , as a practical joke .\n",
      "\n",
      "Decoding: maitre d ' called you christine .\n",
      "Target:   the maitre d ' called you christine .\n",
      "\n",
      "Decoding: i know owner of campton place . i could talk to him in the morning .\n",
      "Target:   i know the owner of campton place . i could talk to him in the morning .\n",
      "\n",
      "Decoding: fresh shirt ...\n",
      "Target:   a fresh shirt ...\n",
      "\n",
      "Decoding: investment banking . moving money from a place to place .\n",
      "Target:   investment banking . moving money from place to place .\n",
      "\n",
      "Decoding: what 's the c .r .s . ?\n",
      "Target:   what 's c .r .s . ?\n",
      "\n",
      "Decoding: this is a c .r .s .\n",
      "Target:   this is c .r .s .\n",
      "\n",
      "Decoding: their ladder here .\n",
      "Target:   there 's a ladder here .\n",
      "\n",
      "Decoding: this is n't attempt to be gallant . if i do n't lift you , how are you going to get there ?\n",
      "Target:   this is n't an attempt to be gallant . if i do n't lift you , how are you going to get there ?\n",
      "\n",
      "Decoding: are you suggesting we wait till someone 's finds us ?\n",
      "Target:   are you suggesting we wait till someone finds us ?\n",
      "\n",
      "Decoding: `` ... wait for help . '' wait for help . i 'm not opening that specifically warns me not to .\n",
      "Target:   `` ... wait for help . '' wait for help . i 'm not opening a door that specifically warns me not to .\n",
      "\n",
      "Decoding: read what it says : `` warning , do < u > not < /u > attempt to open . if elevator stops , use the emergency ... ``\n",
      "Target:   read what it says : `` warning , do < u > not < /u > attempt to open . if elevator stops , use emergency ... ``\n",
      "\n",
      "Decoding: long story . i found this key in the mouth of wooden harlequin .\n",
      "Target:   long story . i found this key in the mouth of a wooden harlequin .\n",
      "\n",
      "Decoding: how do you know that way ?\n",
      "Target:   how do you know that 's the way ?\n",
      "\n",
      "Decoding: it 's run by company ... they play elaborate pranks . things like this . i 'm really only now finding out myself .\n",
      "Target:   it 's run by a company ... they play elaborate pranks . things like this . i 'm really only now finding out myself .\n",
      "\n",
      "Decoding: you got to be kidding .\n",
      "Target:   you 've got to be kidding .\n",
      "\n",
      "Decoding: i do n't think he breathing .\n",
      "Target:   i do n't think he 's breathing .\n",
      "\n",
      "Decoding: a bad month . you did exact the same thing to me last week .\n",
      "Target:   a bad month . you did the exact same thing to me last week .\n",
      "\n",
      "Decoding: yeah , yeah . she 's called a cab . said something about catching plane .\n",
      "Target:   yeah , yeah . she called a cab . said something about catching a plane .\n",
      "\n",
      "Decoding: oh , god yes please . thanks , man . i take you up on that .\n",
      "Target:   oh , god yes please . thanks , man . i 'll take you up on that .\n",
      "\n",
      "Decoding: this ... ? oh , this is just ... this is bill .\n",
      "Target:   this ... ? oh , this is just ... this is the bill .\n",
      "\n",
      "Decoding: baby , they were all over the house with metal detectors . they switched your gun with look-alike , rigged barrel , loaded with blanks . pop-gun .\n",
      "Target:   baby , they were all over the house with metal detectors . they switched your gun with a look-alike , rigged barrel , loaded with blanks . pop-gun .\n",
      "\n",
      "Decoding: you dodged bullet .\n",
      "Target:   you dodged a bullet .\n",
      "\n",
      "Decoding: c .r .s . who do you think ? jesus h . , thank your lucky charms . to think what i 've almost got you into .\n",
      "Target:   c .r .s . who do you think ? jesus h . , thank your lucky charms . to think what i almost got you into .\n",
      "\n",
      "Decoding: it 's profound life experience .\n",
      "Target:   it 's a profound life experience .\n",
      "\n",
      "Decoding: you 've heard of it . you 've seen other people having it . they 're entertainment service , but more than that .\n",
      "Target:   you 've heard of it . you 've seen other people having it . they 're an entertainment service , but more than that .\n",
      "\n",
      "Decoding: they make your life fun . there 's only guarantee is you will not be bored .\n",
      "Target:   they make your life fun . their only guarantee is you will not be bored .\n",
      "\n",
      "Decoding: not after i done with it . actually , i 've been here . in grad-school i bought crystal-meth from the maitre d ' .\n",
      "Target:   not after i 'm done with it . actually , i 've been here . in grad-school i bought crystal-meth from the maitre d ' .\n",
      "\n",
      "Decoding: that 's why it 's a classic . come on , man ... how 'bout hug ... ?\n",
      "Target:   that 's why it 's a classic . come on , man ... how 'bout a hug ... ?\n",
      "\n",
      "Decoding: how much is it ? a few thousand , at least . a rolex like that ... lucky for you 've missed it .\n",
      "Target:   how much is it ? a few thousand , at least . a rolex like that ... lucky for you they missed it .\n",
      "\n",
      "Decoding: i told you , they hired me over the phone . i 've never met anyone .\n",
      "Target:   i told you , they hired me over the phone . i never met anyone .\n",
      "\n",
      "Decoding: i do n't want money . i 'm pulling back curtain . i 'm here to meet the wizard .\n",
      "Target:   i do n't want money . i 'm pulling back the curtain . i 'm here to meet the wizard .\n",
      "\n",
      "Decoding: tell them the cops are after you ... tell them you got to talk to someone , i 'm threatening to blow the whistle .\n",
      "Target:   tell them the cops are after you ... tell them you 've got to talk to someone , i 'm threatening to blow the whistle .\n",
      "\n",
      "Decoding: they own the whole building . they just move from the floor to floor .\n",
      "Target:   they own the whole building . they just move from floor to floor .\n",
      "\n",
      "Decoding: look , it was just a job . nothing personal , ya know ? i play my part , improvise little . that 's what i 'm good at .\n",
      "Target:   look , it was just a job . nothing personal , ya know ? i play my part , improvise a little . that 's what i 'm good at .\n",
      "\n",
      "Decoding: that 's right -- you 're left-brain the word fetishist .\n",
      "Target:   that 's right -- you 're a left-brain word fetishist .\n",
      "\n",
      "Decoding: one guarantee . payment 's entirely at your brother discretion and , as a gift , dependent on your satisfaction .\n",
      "Target:   one guarantee . payment 's entirely at your brother 's discretion and , as a gift , dependent on your satisfaction .\n",
      "\n",
      "Decoding: your brother was a client with our branch . we do a sort of informal scoring . his numbers were outstanding . sure you 're not hungry at all ... ? tung hoy , best in chinatown ...\n",
      "Target:   your brother was a client with our london branch . we do a sort of informal scoring . his numbers were outstanding . sure you 're not hungry at all ... ? tung hoy , best in chinatown ...\n",
      "\n",
      "Decoding: key ?\n",
      "Target:   the key ?\n",
      "\n",
      "Decoding: nobody 's worried about your father .\n",
      "Target:   nobody worried about your father .\n",
      "\n",
      "Decoding: there 's been a break in . lock this door and stay here . do n't move muscle .\n",
      "Target:   there 's been a break in . lock this door and stay here . do n't move a muscle .\n",
      "\n",
      "Decoding: i do n't know what you 're talking about . what happened ?\n",
      "Target:   i do n't know what you 're talking about . what 's happened ?\n",
      "\n",
      "Decoding: did alarm go off ? the house ... they ... you did n't see ... ?\n",
      "Target:   did the alarm go off ? the house ... they ... you did n't see ... ?\n",
      "\n",
      "Decoding: then then .\n",
      "Target:   goodnight then .\n",
      "\n",
      "Decoding: okay . i think he into some sort of new personal improvement cult .\n",
      "Target:   okay . i think he 's into some sort of new personal improvement cult .\n",
      "\n",
      "Decoding: dinner in the oven .\n",
      "Target:   dinner 's in the oven .\n",
      "\n",
      "Decoding: there was incident a few days ago ... a nervous breakdown , they said . the police took him . they left this address , in case anyone ...\n",
      "Target:   there was an incident a few days ago ... a nervous breakdown , they said . the police took him . they left this address , in case anyone ...\n",
      "\n",
      "Decoding: what 's trouble ?\n",
      "Target:   what 's the trouble ?\n",
      "\n",
      "Decoding: mister ... seymour butts .\n",
      "Target:   a mister ... seymour butts .\n",
      "\n",
      "Decoding: what 's the gentleman , maria ?\n",
      "Target:   what gentleman , maria ?\n",
      "\n",
      "Decoding: i would n't mention following , except he was very insistent . it 's obviously some sort of prank ...\n",
      "Target:   i would n't mention the following , except he was very insistent . it 's obviously some sort of prank ...\n",
      "\n",
      "Decoding: i send your regrets . honestly , why must i even bother ?\n",
      "Target:   i 'll send your regrets . honestly , why must i even bother ?\n",
      "\n",
      "Decoding: the hinchberger 's wedding .\n",
      "Target:   the hinchberger wedding .\n",
      "\n",
      "Decoding: invitations : museum gala .\n",
      "Target:   invitations : the museum gala .\n",
      "\n",
      "Decoding: nice touch . does a game use real bullets ... ?\n",
      "Target:   nice touch . does the game use real bullets ... ?\n",
      "\n",
      "Decoding: it 's what they do . it 's like ... being toyed with by a bunch of ... depraved children\n",
      "Target:   it 's what they do . it 's like ... being toyed with by a bunch of ... depraved children .\n",
      "\n",
      "Decoding: find out about a company called the c .r .s . consumer recreation services .\n",
      "Target:   find out about a company called c .r .s . consumer recreation services .\n",
      "\n",
      "Decoding: someone 's playing hardball . it 's complicated . can i ask favor ?\n",
      "Target:   someone 's playing hardball . it 's complicated . can i ask a favor ?\n",
      "\n",
      "Decoding: how 's the concerned should i be ?\n",
      "Target:   how concerned should i be ?\n",
      "\n",
      "Decoding: that you 've a involved conrad ... is unforgivable . i am now your enemy .\n",
      "Target:   that you 've involved conrad ... is unforgivable . i am now your enemy .\n",
      "\n",
      "Decoding: what happened ...\n",
      "Target:   what 's happened ...\n",
      "\n",
      "Decoding: modelling small-group dynamics in formation of narrative hallucinations . you brought us here to scare us . insomnia , that was just a decoy issue . you 're disgusting .\n",
      "Target:   modelling small-group dynamics in the formation of narrative hallucinations . you brought us here to scare us . insomnia , that was just a decoy issue . you 're disgusting .\n",
      "\n",
      "Decoding: come on . these are the typically sentimental gestures of depraved industrialist .\n",
      "Target:   come on . these are the typically sentimental gestures of a depraved industrialist .\n",
      "\n",
      "Decoding: the children . children hugh crain built the house for . the children he never had .\n",
      "Target:   the children . the children hugh crain built the house for . the children he never had .\n",
      "\n",
      "Decoding: obsessive worrier . join club . and you ? i 'd guess ...\n",
      "Target:   obsessive worrier . join the club . and you ? i 'd guess ...\n",
      "\n",
      "Decoding: so why did you need the addam family mansion for a scientific test ?\n",
      "Target:   so why did you need the addam 's family mansion for a scientific test ?\n",
      "\n",
      "Decoding: -- how much is this car 's worth ?\n",
      "Target:   -- how much is this car worth ?\n",
      "\n",
      "Decoding: you do n't really believe it haunted ... do you believe in ghosts ?\n",
      "Target:   you do n't really believe it 's haunted ... do you believe in ghosts ?\n",
      "\n",
      "Decoding: so could you ! is this some fucked up the idea of art , putting someone else 's name to a painting ?\n",
      "Target:   so could you ! is this some fucked up idea of art , putting someone else 's name to a painting ?\n",
      "\n",
      "Decoding: and why did n't marrow tell < u > us < /u > ? does n't he a trust women ? that fuck .\n",
      "Target:   and why did n't marrow tell < u > us < /u > ? does n't he trust women ? that fuck .\n",
      "\n",
      "Decoding: nah , you 're going crazy with doubt , all of your mistakes are coming back up the pipes , and it 's worse than nightmare . --\n",
      "Target:   nah , you 're going crazy with doubt , all of your mistakes are coming back up the pipes , and it 's worse than a nightmare . --\n",
      "\n",
      "Decoding: not the way you 've constructed your group , it just not ethical !\n",
      "Target:   not the way you 've constructed your group , it 's just not ethical !\n",
      "\n",
      "Decoding: children want me . they 're calling me . they need me .\n",
      "Target:   the children want me . they 're calling me . they need me .\n",
      "\n",
      "Decoding: i looked at theo . she had look on her face .\n",
      "Target:   i looked at theo . she had a look on her face .\n",
      "\n",
      "Decoding: i was n't thinking about my mother bathroom .\n",
      "Target:   i was n't thinking about my mother 's bathroom .\n",
      "\n",
      "Decoding: so ... smell ... is ... smell is sense that triggers the most powerful memories . and memory can trigger a smell .\n",
      "Target:   so ... smell ... is ... smell is the sense that triggers the most powerful memories . and a memory can trigger a smell .\n",
      "\n",
      "Decoding: in the bathroom in my mother 's room , toilet was next to old wooden table . it smelled like that wood .\n",
      "Target:   in the bathroom in my mother 's room , the toilet was next to an old wooden table . it smelled like that wood .\n",
      "\n",
      "Decoding: cold sensation . who felt it first ?\n",
      "Target:   the cold sensation . who felt it first ?\n",
      "\n",
      "Decoding: i really ... honored to be part of this study , jim .\n",
      "Target:   i 'm really ... honored to be part of this study , jim .\n",
      "\n",
      "Decoding: nell . good enough . and i jim .\n",
      "Target:   nell . good enough . and i 'm jim .\n",
      "\n",
      "Decoding: that ? that 's a hill house .\n",
      "Target:   that ? that 's hill house .\n",
      "\n",
      "Decoding: here 's how they 're organized . groups of five , very different personalities : scored all over the kiersey temperament sorter just like you asked for . and they all score high on insomnia charts .\n",
      "Target:   here 's how they 're organized . groups of five , very different personalities : scored all over the kiersey temperament sorter just like you asked for . and they all score high on the insomnia charts .\n",
      "\n",
      "Decoding: you hear the vibrations in the wire . there 's magnetic pulse in the wires , you feel it . i could test it .\n",
      "Target:   you hear the vibrations in the wire . there 's a magnetic pulse in the wires , you feel it . i could test it .\n",
      "\n",
      "Decoding: but experiment was a failure .\n",
      "Target:   but the experiment was a failure .\n",
      "\n",
      "Decoding: he wandering around house , and nell heard him . she thought it was ghosts . let 's go look for him again .\n",
      "Target:   he 's wandering around the house , and nell heard him . she thought it was ghosts . let 's go look for him again .\n",
      "\n",
      "Decoding: i 'll take her with me to university tomorrow . i ca n't believe i read the test wrong . i did n't see anything that looked like she was suicidal .\n",
      "Target:   i 'll take her with me to the university tomorrow . i ca n't believe i read the test wrong . i did n't see anything that looked like she was suicidal .\n",
      "\n",
      "Decoding: no , but nell been here longer than i have .\n",
      "Target:   no , but nell 's been here longer than i have .\n",
      "\n",
      "Decoding: rene crain . up there . rope . ship 's hawser . hard to tie . do n't know how she 's got it .\n",
      "Target:   rene crain . up there . rope . ship 's hawser . hard to tie . do n't know how she got it .\n",
      "\n",
      "Decoding: mrs . dudley be waiting for you .\n",
      "Target:   mrs . dudley 'll be waiting for you .\n",
      "\n",
      "Decoding: that 's a good question . what is it about fences ? sometimes a locked chain makes people on both sides of fence just a little more comfortable . why would that be ?\n",
      "Target:   that 's a good question . what is it about fences ? sometimes a locked chain makes people on both sides of the fence just a little more comfortable . why would that be ?\n",
      "\n",
      "Decoding: well , i 've never lived with a beauty . you must love working here .\n",
      "Target:   well , i 've never lived with beauty . you must love working here .\n",
      "\n",
      "Decoding: nell , it makes sense . it 's all makes sense . you and i , we were scaring each other , working each other up .\n",
      "Target:   nell , it makes sense . it all makes sense . you and i , we were scaring each other , working each other up .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for decoding, target in errors:\n",
    "    print(\"Decoding: \" + \" \".join(decoding))\n",
    "    print(\"Target:   \" + \" \".join(target) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
