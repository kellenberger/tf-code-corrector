{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import nltk\n",
    "\n",
    "from correct_text import train, decode, decode_sentence, evaluate_accuracy, create_model,\\\n",
    "    get_corrective_tokens, DefaultPTBConfig, DefaultMovieDialogConfig\n",
    "from text_corrector_data_readers import PTBDataReader, MovieDialogReader\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data_path = \"C:\\\\Users\\\\svenk\\\\Documents\\\\dialog_corpus\"\n",
    "# root_data_path = \"E:\\\\Documents\\\\dialog_corpus\"\n",
    "full_path = os.path.join(root_data_path, \"movie_lines.txt\")\n",
    "train_path = os.path.join(root_data_path, \"train.txt\")\n",
    "val_path = os.path.join(root_data_path, \"val.txt\")\n",
    "test_path = os.path.join(root_data_path, \"test.txt\")\n",
    "model_path = os.path.join(root_data_path, \"dialog_correcter_model_testnltk\")\n",
    "config = DefaultMovieDialogConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Lines: 213299/304713, 69.99996718223377%\n",
      "Val Lines: 45707/304713, 15.000016408883113%\n",
      "Test Lines: 45707/304713, 15.000016408883113%\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "with open(full_path, \"r\") as raw_data:\n",
    "        for line in raw_data:\n",
    "            count += 1\n",
    "train_count = 0\n",
    "val_count = 0\n",
    "test_count = 0\n",
    "with open(full_path, \"r\") as raw_data, \\\n",
    "            open(train_path, \"w\") as train, \\\n",
    "            open(val_path, \"w\") as val, \\\n",
    "            open(test_path, \"w\") as test:\n",
    "        for i, line in enumerate(raw_data):\n",
    "            if i+1 < 0.7*count:\n",
    "                train.write(line)\n",
    "                train_count += 1\n",
    "            elif i+1 < 0.85*count:\n",
    "                val.write(line)\n",
    "                val_count += 1\n",
    "            else:\n",
    "                test.write(line)\n",
    "                test_count += 1\n",
    "print(\"Train Lines: {}/{}, {}%\".format(train_count, count, train_count * 100.0 / count))\n",
    "print(\"Val Lines: {}/{}, {}%\".format(val_count, count, val_count * 100.0 / count))\n",
    "print(\"Test Lines: {}/{}, {}%\".format(test_count, count, test_count * 100.0 / count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reader = MovieDialogReader(config, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data; train = C:\\Users\\svenk\\Documents\\dialog_corpus\\train.txt, test = C:\\Users\\svenk\\Documents\\dialog_corpus\\val.txt\n",
      "Creating 4 layers of 512 units.\n",
      "Created model with fresh parameters.\n",
      "Training bucket sizes: [198232, 85878, 50002, 70314]\n",
      "Total train size: 404426.0\n",
      "global step 100 learning rate 0.5000 step-time 10.70 perplexity 300.21\n",
      "  eval: bucket 0 perplexity 93.15\n",
      "  eval: bucket 1 perplexity 134.16\n",
      "  eval: bucket 2 perplexity 145.36\n",
      "  eval: bucket 3 perplexity 170.63\n",
      "global step 200 learning rate 0.5000 step-time 10.00 perplexity 120.32\n",
      "  eval: bucket 0 perplexity 72.97\n",
      "  eval: bucket 1 perplexity 138.97\n",
      "  eval: bucket 2 perplexity 157.70\n",
      "  eval: bucket 3 perplexity 174.74\n",
      "global step 300 learning rate 0.5000 step-time 10.22 perplexity 93.21\n",
      "  eval: bucket 0 perplexity 48.15\n",
      "  eval: bucket 1 perplexity 100.84\n",
      "  eval: bucket 2 perplexity 143.31\n",
      "  eval: bucket 3 perplexity 162.79\n",
      "global step 400 learning rate 0.5000 step-time 10.13 perplexity 79.39\n",
      "  eval: bucket 0 perplexity 43.34\n",
      "  eval: bucket 1 perplexity 114.30\n",
      "  eval: bucket 2 perplexity 144.42\n",
      "  eval: bucket 3 perplexity 161.30\n",
      "global step 500 learning rate 0.5000 step-time 12.82 perplexity 74.28\n",
      "  eval: bucket 0 perplexity 37.39\n",
      "  eval: bucket 1 perplexity 73.30\n",
      "  eval: bucket 2 perplexity 108.09\n",
      "  eval: bucket 3 perplexity 141.16\n",
      "global step 600 learning rate 0.5000 step-time 11.83 perplexity 59.00\n",
      "  eval: bucket 0 perplexity 26.61\n",
      "  eval: bucket 1 perplexity 76.22\n",
      "  eval: bucket 2 perplexity 101.09\n",
      "  eval: bucket 3 perplexity 111.66\n",
      "global step 700 learning rate 0.5000 step-time 11.15 perplexity 44.69\n",
      "  eval: bucket 0 perplexity 19.45\n",
      "  eval: bucket 1 perplexity 71.48\n",
      "  eval: bucket 2 perplexity 99.36\n",
      "  eval: bucket 3 perplexity 129.86\n",
      "global step 800 learning rate 0.5000 step-time 12.53 perplexity 42.47\n",
      "  eval: bucket 0 perplexity 24.41\n",
      "  eval: bucket 1 perplexity 155.40\n",
      "  eval: bucket 2 perplexity 209.61\n",
      "  eval: bucket 3 perplexity 288.92\n",
      "global step 900 learning rate 0.5000 step-time 14.04 perplexity 39.72\n",
      "  eval: bucket 0 perplexity 10.73\n",
      "  eval: bucket 1 perplexity 44.46\n",
      "  eval: bucket 2 perplexity 61.29\n",
      "  eval: bucket 3 perplexity 88.76\n",
      "global step 1000 learning rate 0.5000 step-time 11.57 perplexity 24.56\n",
      "  eval: bucket 0 perplexity 13.27\n",
      "  eval: bucket 1 perplexity 44.80\n",
      "  eval: bucket 2 perplexity 61.81\n",
      "  eval: bucket 3 perplexity 73.57\n",
      "global step 1100 learning rate 0.5000 step-time 14.15 perplexity 26.04\n",
      "  eval: bucket 0 perplexity 9.24\n",
      "  eval: bucket 1 perplexity 24.51\n",
      "  eval: bucket 2 perplexity 48.21\n",
      "  eval: bucket 3 perplexity 64.06\n",
      "global step 1200 learning rate 0.5000 step-time 11.90 perplexity 20.21\n",
      "  eval: bucket 0 perplexity 8.09\n",
      "  eval: bucket 1 perplexity 31.35\n",
      "  eval: bucket 2 perplexity 50.57\n",
      "  eval: bucket 3 perplexity 84.57\n",
      "global step 1300 learning rate 0.5000 step-time 12.63 perplexity 18.82\n",
      "  eval: bucket 0 perplexity 6.39\n",
      "  eval: bucket 1 perplexity 21.11\n",
      "  eval: bucket 2 perplexity 41.42\n",
      "  eval: bucket 3 perplexity 79.34\n",
      "global step 1400 learning rate 0.5000 step-time 12.24 perplexity 13.26\n",
      "  eval: bucket 0 perplexity 5.27\n",
      "  eval: bucket 1 perplexity 32.26\n",
      "  eval: bucket 2 perplexity 59.83\n",
      "  eval: bucket 3 perplexity 70.95\n",
      "global step 1500 learning rate 0.5000 step-time 12.10 perplexity 11.71\n",
      "  eval: bucket 0 perplexity 4.06\n",
      "  eval: bucket 1 perplexity 11.08\n",
      "  eval: bucket 2 perplexity 21.31\n",
      "  eval: bucket 3 perplexity 44.20\n",
      "global step 1600 learning rate 0.5000 step-time 11.72 perplexity 8.56\n",
      "  eval: bucket 0 perplexity 2.95\n",
      "  eval: bucket 1 perplexity 10.32\n",
      "  eval: bucket 2 perplexity 14.05\n",
      "  eval: bucket 3 perplexity 34.55\n",
      "global step 1700 learning rate 0.5000 step-time 13.04 perplexity 7.49\n",
      "  eval: bucket 0 perplexity 2.12\n",
      "  eval: bucket 1 perplexity 6.32\n",
      "  eval: bucket 2 perplexity 10.52\n",
      "  eval: bucket 3 perplexity 18.00\n",
      "global step 1800 learning rate 0.5000 step-time 13.04 perplexity 6.64\n",
      "  eval: bucket 0 perplexity 2.81\n",
      "  eval: bucket 1 perplexity 8.33\n",
      "  eval: bucket 2 perplexity 12.88\n",
      "  eval: bucket 3 perplexity 36.71\n",
      "global step 1900 learning rate 0.5000 step-time 13.76 perplexity 5.23\n",
      "  eval: bucket 0 perplexity 2.15\n",
      "  eval: bucket 1 perplexity 3.84\n",
      "  eval: bucket 2 perplexity 6.35\n",
      "  eval: bucket 3 perplexity 11.65\n",
      "global step 2000 learning rate 0.5000 step-time 11.42 perplexity 3.35\n",
      "  eval: bucket 0 perplexity 2.23\n",
      "  eval: bucket 1 perplexity 3.62\n",
      "  eval: bucket 2 perplexity 4.86\n",
      "  eval: bucket 3 perplexity 11.23\n",
      "global step 2100 learning rate 0.5000 step-time 12.59 perplexity 3.16\n",
      "  eval: bucket 0 perplexity 2.18\n",
      "  eval: bucket 1 perplexity 2.86\n",
      "  eval: bucket 2 perplexity 4.76\n",
      "  eval: bucket 3 perplexity 11.32\n",
      "global step 2200 learning rate 0.5000 step-time 13.63 perplexity 3.05\n",
      "  eval: bucket 0 perplexity 2.26\n",
      "  eval: bucket 1 perplexity 2.57\n",
      "  eval: bucket 2 perplexity 3.58\n",
      "  eval: bucket 3 perplexity 5.53\n",
      "global step 2300 learning rate 0.5000 step-time 14.71 perplexity 2.83\n",
      "  eval: bucket 0 perplexity 1.55\n",
      "  eval: bucket 1 perplexity 3.10\n",
      "  eval: bucket 2 perplexity 3.49\n",
      "  eval: bucket 3 perplexity 4.53\n",
      "global step 2400 learning rate 0.5000 step-time 12.32 perplexity 2.47\n",
      "  eval: bucket 0 perplexity 1.50\n",
      "  eval: bucket 1 perplexity 2.23\n",
      "  eval: bucket 2 perplexity 2.55\n",
      "  eval: bucket 3 perplexity 3.73\n",
      "global step 2500 learning rate 0.5000 step-time 12.09 perplexity 2.17\n",
      "  eval: bucket 0 perplexity 1.56\n",
      "  eval: bucket 1 perplexity 2.26\n",
      "  eval: bucket 2 perplexity 3.38\n",
      "  eval: bucket 3 perplexity 9.13\n",
      "global step 2600 learning rate 0.5000 step-time 13.38 perplexity 2.45\n",
      "  eval: bucket 0 perplexity 1.78\n",
      "  eval: bucket 1 perplexity 2.75\n",
      "  eval: bucket 2 perplexity 3.58\n",
      "  eval: bucket 3 perplexity 5.86\n",
      "global step 2700 learning rate 0.5000 step-time 11.46 perplexity 1.92\n",
      "  eval: bucket 0 perplexity 1.23\n",
      "  eval: bucket 1 perplexity 2.12\n",
      "  eval: bucket 2 perplexity 2.13\n",
      "  eval: bucket 3 perplexity 3.65\n",
      "global step 2800 learning rate 0.5000 step-time 12.43 perplexity 2.29\n",
      "  eval: bucket 0 perplexity 1.41\n",
      "  eval: bucket 1 perplexity 2.01\n",
      "  eval: bucket 2 perplexity 2.29\n",
      "  eval: bucket 3 perplexity 3.42\n",
      "global step 2900 learning rate 0.5000 step-time 13.72 perplexity 2.27\n",
      "  eval: bucket 0 perplexity 1.38\n",
      "  eval: bucket 1 perplexity 1.89\n",
      "  eval: bucket 2 perplexity 2.28\n",
      "  eval: bucket 3 perplexity 3.27\n",
      "global step 3000 learning rate 0.5000 step-time 11.21 perplexity 1.82\n",
      "  eval: bucket 0 perplexity 1.29\n",
      "  eval: bucket 1 perplexity 1.60\n",
      "  eval: bucket 2 perplexity 1.94\n",
      "  eval: bucket 3 perplexity 3.52\n",
      "global step 3100 learning rate 0.5000 step-time 12.93 perplexity 1.97\n",
      "  eval: bucket 0 perplexity 1.36\n",
      "  eval: bucket 1 perplexity 1.57\n",
      "  eval: bucket 2 perplexity 2.21\n",
      "  eval: bucket 3 perplexity 2.80\n",
      "global step 3200 learning rate 0.5000 step-time 12.34 perplexity 1.75\n",
      "  eval: bucket 0 perplexity 1.35\n",
      "  eval: bucket 1 perplexity 1.99\n",
      "  eval: bucket 2 perplexity 2.27\n",
      "  eval: bucket 3 perplexity 6.32\n",
      "global step 3300 learning rate 0.5000 step-time 12.28 perplexity 1.86\n",
      "  eval: bucket 0 perplexity 1.24\n",
      "  eval: bucket 1 perplexity 1.76\n",
      "  eval: bucket 2 perplexity 2.57\n",
      "  eval: bucket 3 perplexity 8.15\n",
      "global step 3400 learning rate 0.5000 step-time 13.28 perplexity 2.19\n",
      "  eval: bucket 0 perplexity 1.51\n",
      "  eval: bucket 1 perplexity 1.86\n",
      "  eval: bucket 2 perplexity 2.41\n",
      "  eval: bucket 3 perplexity 4.91\n",
      "global step 3500 learning rate 0.4950 step-time 11.05 perplexity 1.69\n",
      "  eval: bucket 0 perplexity 1.31\n",
      "  eval: bucket 1 perplexity 2.03\n",
      "  eval: bucket 2 perplexity 2.76\n",
      "  eval: bucket 3 perplexity 5.57\n",
      "global step 3600 learning rate 0.4950 step-time 12.39 perplexity 1.78\n",
      "  eval: bucket 0 perplexity 1.23\n",
      "  eval: bucket 1 perplexity 1.68\n",
      "  eval: bucket 2 perplexity 2.69\n",
      "  eval: bucket 3 perplexity 4.61\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-6d7a3268a1b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_reader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\deep-code-corrector\\correct_text.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(data_reader, train_path, test_path, model_path)\u001b[0m\n\u001b[0;32m    179\u001b[0m                 train_data, bucket_id)\n\u001b[0;32m    180\u001b[0m             _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n\u001b[1;32m--> 181\u001b[1;33m                                          target_weights, bucket_id, False)\n\u001b[0m\u001b[0;32m    182\u001b[0m             \u001b[0mstep_time\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[1;33m.\u001b[0m\u001b[0msteps_per_checkpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\deep-code-corrector\\text_corrector_models.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, session, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only, corrective_tokens)\u001b[0m\n\u001b[0;32m    286\u001b[0m                 \u001b[0moutput_feed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_feed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mforward_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[1;31m# Gradient norm, loss, no outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "train(data_reader, train_path, val_path, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reader = MovieDialogReader(config, train_path, dropout_prob=0.25, replacement_prob=0.25, dataset_copies=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrective_tokens = get_corrective_tokens(data_reader, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(root_data_path, \"corrective_tokens.pickle\"), \"wb\") as f:\n",
    "    pickle.dump(corrective_tokens, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(os.path.join(root_data_path, \"token_to_id.pickle\"), \"wb\") as f:\n",
    "    pickle.dump(data_reader.token_to_id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model with fresh parameters.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = create_model(sess, True, model_path, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: you must have girlfriend\n",
      "Output: must must must must must must must must must must\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test a sample from the test dataset.\n",
    "decoded = decode_sentence(sess, model, data_reader, \"you must have girlfriend\", corrective_tokens=corrective_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['must',\n",
       " 'must',\n",
       " 'must',\n",
       " 'must',\n",
       " 'must',\n",
       " 'must',\n",
       " 'must',\n",
       " 'must',\n",
       " 'must',\n",
       " 'must']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: did n't you say that they 're going to develop this revolutionary new thing ...\n",
      "Output: that that that that that that that that that that that that that that that that that that that that\n",
      "\n"
     ]
    }
   ],
   "source": [
    "decoded = decode_sentence(sess, model, data_reader,\n",
    "                          \"did n't you say that they 're going to develop this revolutionary new thing ...\",\n",
    "                          corrective_tokens=corrective_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['went',\n",
       " 'went',\n",
       " 'went',\n",
       " 'went',\n",
       " 'went',\n",
       " 'than',\n",
       " 'than',\n",
       " 'than',\n",
       " 'than',\n",
       " 'than']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(sess, model, data_reader, \"kvothe went to market\", corrective_tokens=corrective_tokens, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'ll\", \"'ll\", \"'ll\", \"'ll\", \"'ll\", \"'ll\", \"'ll\", 'PAD', 'PAD', 'PAD']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(sess, model, data_reader, \"blablahblah and bladdddd went to market\", corrective_tokens=corrective_tokens,\n",
    "                verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'ll\", \"'ll\", \"'m\", \"'m\", \"'m\", \"'m\", \"'m\", \"'m\", \"'m\", \"'m\"]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(sess, model, data_reader, \"do you have book\", corrective_tokens=corrective_tokens, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'s\", \"'s\", \"'s\", \"'s\", 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(sess, model, data_reader, \"the cardinals did better then the cubs\", corrective_tokens=corrective_tokens, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-3ac453e69476>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 4 layers, 40k steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_reader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorrective_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, max_samples=1000)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\deep-code-corrector\\correct_text.py\u001b[0m in \u001b[0;36mevaluate_accuracy\u001b[1;34m(sess, model, data_reader, corrective_tokens, test_path, max_samples)\u001b[0m\n\u001b[0;32m    354\u001b[0m         decoding = next(\n\u001b[0;32m    355\u001b[0m             decode(sess, model, data_reader, [source],\n\u001b[1;32m--> 356\u001b[1;33m                    corrective_tokens=corrective_tokens, verbose=False))\n\u001b[0m\u001b[0;32m    357\u001b[0m         \u001b[0mmodel_hypotheses\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdecoding\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\deep-code-corrector\\correct_text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(sess, model, data_reader, data_to_decode, corrective_tokens, verbose)\u001b[0m\n\u001b[0;32m    274\u001b[0m         _, _, output_logits = model.step(\n\u001b[0;32m    275\u001b[0m             \u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbucket_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m             True, corrective_tokens=corrective_tokens_mask)\n\u001b[0m\u001b[0;32m    277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         oov_input_tokens = [token for token in tokens if\n",
      "\u001b[1;32m~\\deep-code-corrector\\text_corrector_models.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, session, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only, corrective_tokens)\u001b[0m\n\u001b[0;32m    286\u001b[0m                 \u001b[0moutput_feed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_feed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mforward_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[1;31m# Gradient norm, loss, no outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1135\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1137\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1138\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1353\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1355\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1356\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1357\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1359\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[1;32m-> 1340\u001b[1;33m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4 layers, 40k steps\n",
    "errors = evaluate_accuracy(sess, model, data_reader, corrective_tokens, test_path)#, max_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 0: (10, 10)\n",
      "\tBaseline BLEU = 0.8368\n",
      "\tModel BLEU = 0.8425\n",
      "\tBaseline Accuracy: 0.9110\n",
      "\tModel Accuracy: 0.9303\n",
      "Bucket 1: (15, 15)\n",
      "\tBaseline BLEU = 0.8818\n",
      "\tModel BLEU = 0.8459\n",
      "\tBaseline Accuracy: 0.8063\n",
      "\tModel Accuracy: 0.8014\n",
      "Bucket 2: (20, 20)\n",
      "\tBaseline BLEU = 0.8891\n",
      "\tModel BLEU = 0.7986\n",
      "\tBaseline Accuracy: 0.7309\n",
      "\tModel Accuracy: 0.6281\n",
      "Bucket 3: (40, 40)\n",
      "\tBaseline BLEU = 0.9099\n",
      "\tModel BLEU = 0.5997\n",
      "\tBaseline Accuracy: 0.6007\n",
      "\tModel Accuracy: 0.1607\n"
     ]
    }
   ],
   "source": [
    "# 4 layers, 30k steps\n",
    "errors = evaluate_accuracy(sess, model, data_reader, corrective_tokens, test_path)#, max_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 0: (10, 10)\n",
      "\tBaseline BLEU = 0.8330\n",
      "\tModel BLEU = 0.8335\n",
      "\tBaseline Accuracy: 0.9067\n",
      "\tModel Accuracy: 0.9218\n",
      "Bucket 1: (15, 15)\n",
      "\tBaseline BLEU = 0.8772\n",
      "\tModel BLEU = 0.8100\n",
      "\tBaseline Accuracy: 0.7980\n",
      "\tModel Accuracy: 0.7437\n",
      "Bucket 2: (20, 20)\n",
      "\tBaseline BLEU = 0.8898\n",
      "\tModel BLEU = 0.7636\n",
      "\tBaseline Accuracy: 0.7366\n",
      "\tModel Accuracy: 0.5370\n",
      "Bucket 3: (40, 40)\n",
      "\tBaseline BLEU = 0.9098\n",
      "\tModel BLEU = 0.5387\n",
      "\tBaseline Accuracy: 0.6041\n",
      "\tModel Accuracy: 0.1117\n"
     ]
    }
   ],
   "source": [
    "# 4 layers, 20k steps\n",
    "errors = evaluate_accuracy(sess, model, data_reader, corrective_tokens, test_path)#, max_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 0: (10, 10)\n",
      "\tBaseline BLEU = 0.8341\n",
      "\tModel BLEU = 0.8516\n",
      "\tBaseline Accuracy: 0.9083\n",
      "\tModel Accuracy: 0.9384\n",
      "Bucket 1: (15, 15)\n",
      "\tBaseline BLEU = 0.8850\n",
      "\tModel BLEU = 0.8860\n",
      "\tBaseline Accuracy: 0.8156\n",
      "\tModel Accuracy: 0.8491\n",
      "Bucket 2: (20, 20)\n",
      "\tBaseline BLEU = 0.8876\n",
      "\tModel BLEU = 0.8880\n",
      "\tBaseline Accuracy: 0.7291\n",
      "\tModel Accuracy: 0.7817\n",
      "Bucket 3: (40, 40)\n",
      "\tBaseline BLEU = 0.9099\n",
      "\tModel BLEU = 0.9045\n",
      "\tBaseline Accuracy: 0.6073\n",
      "\tModel Accuracy: 0.6425\n"
     ]
    }
   ],
   "source": [
    "errors = evaluate_accuracy(sess, model, data_reader, corrective_tokens, test_path)#, max_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding: you beg for mercy in a second .\n",
      "Target:   you 'll beg for mercy in a second .\n",
      "\n",
      "Decoding: i 'm dying for a shower . you could use the one too . and we 'd better check that bandage .\n",
      "Target:   i 'm dying for a shower . you could use one too . and we 'd better check that bandage .\n",
      "\n",
      "Decoding: whatever ... they 've become hotshot computer guys so they get a job to build el computer grande ... skynet ... for the government . right ?\n",
      "Target:   whatever ... they become the hotshot computer guys so they get the job to build el computer grande ... skynet ... for the government . right ?\n",
      "\n",
      "Decoding: did n't you say that they 're going to develop this revolutionary a new thing ...\n",
      "Target:   did n't you say that they 're going to develop this revolutionary new thing ...\n",
      "\n",
      "Decoding: bag some z ?\n",
      "Target:   bag some z 's ?\n",
      "\n",
      "Decoding: sleep . it 'll be a light soon .\n",
      "Target:   sleep . it 'll be light soon .\n",
      "\n",
      "Decoding: well , at least i know what to name him . i do n't suppose you 'd know who father is ? so i do n't tell him to get lost when i meet him .\n",
      "Target:   well , at least i know what to name him . i do n't suppose you 'd know who the father is ? so i do n't tell him to get lost when i meet him .\n",
      "\n",
      "Decoding: we got ta get you to doctor .\n",
      "Target:   we got ta get you to a doctor .\n",
      "\n",
      "Decoding: hunter killers . patrol machines . a build in automated factories . most of us were rounded up , put in camps ... for orderly disposal .\n",
      "Target:   hunter killers . patrol machines . build in automated factories . most of us were rounded up , put in camps ... for orderly disposal .\n",
      "\n",
      "Decoding: but outside , it 's a living human tissue . flesh , skin , hair ... blood . grown for the cyborgs .\n",
      "Target:   but outside , it 's living human tissue . flesh , skin , hair ... blood . grown for the cyborgs .\n",
      "\n",
      "Decoding: you heard enough . decide . are you going to release me ?\n",
      "Target:   you 've heard enough . decide . are you going to release me ?\n",
      "\n",
      "Decoding: okay . okay . but this ... cyborg ... if it metal ...\n",
      "Target:   okay . okay . but this ... cyborg ... if it 's metal ...\n",
      "\n",
      "Decoding: you go naked . something about the field generated by living organism . nothing dead will go .\n",
      "Target:   you go naked . something about the field generated by a living organism . nothing dead will go .\n",
      "\n",
      "Decoding: ca n't . nobody goes home . nobody else comes through . it just him and me .\n",
      "Target:   ca n't . nobody goes home . nobody else comes through . it 's just him and me .\n",
      "\n",
      "Decoding: i see . and this ... computer , thinks it can win by killing the mother of its enemy , kill- ing him , in effect , before he is even conceived ? sort of retroactive abortion ?\n",
      "Target:   i see . and this ... computer , thinks it can win by killing the mother of its enemy , kill- ing him , in effect , before he is even conceived ? a sort of retroactive abortion ?\n",
      "\n",
      "Decoding: skynet . a computer defense system built for sac-norad by cyber dynamics . modified series 4800 .\n",
      "Target:   skynet . a computer defense system built for sac-norad by cyber dynamics . a modified series 4800 .\n",
      "\n",
      "Decoding: a year 2027 ?\n",
      "Target:   the year 2027 ?\n",
      "\n",
      "Decoding: with one thirty a second under perry , from '21 to '27 --\n",
      "Target:   with the one thirty second under perry , from '21 to '27 --\n",
      "\n",
      "Decoding: why do n't you just stretch out here and get some sleep . it take your mom 's a good hour to get here from redlands .\n",
      "Target:   why do n't you just stretch out here and get some sleep . it 'll take your mom a good hour to get here from redlands .\n",
      "\n",
      "Decoding: lieutenant , are you sure it them ? maybe i should see the ... bodies .\n",
      "Target:   lieutenant , are you sure it 's them ? maybe i should see the ... bodies .\n",
      "\n",
      "Decoding: i already did . no answer at the door and the apartment manager 's out . i keeping them there .\n",
      "Target:   i already did . no answer at the door and the apartment manager 's out . i 'm keeping them there .\n",
      "\n",
      "Decoding: that stuff two hours cold .\n",
      "Target:   that stuff 's two hours cold .\n",
      "\n",
      "Decoding: you got ta be kidding me . the new guys 'll be short-stroking it over this one . one-day pattern killer .\n",
      "Target:   you got ta be kidding me . the new guys 'll be short-stroking it over this one . a one-day pattern killer .\n",
      "\n",
      "Decoding: give me a short version .\n",
      "Target:   give me the short version .\n",
      "\n",
      "Decoding: because it 's fair . give me the next quarter . if you still feel this way , vote your shares ...\n",
      "Target:   because it 's fair . give me next quarter . if you still feel this way , vote your shares ...\n",
      "\n",
      "Decoding: it 's probably will . in fact , i 'd go so far as to say it 's almost certainly will , in time . why should i settle for that ?\n",
      "Target:   it probably will . in fact , i 'd go so far as to say it almost certainly will , in time . why should i settle for that ?\n",
      "\n",
      "Decoding: stock will turn .\n",
      "Target:   the stock will turn .\n",
      "\n",
      "Decoding: you want to know what it is ? what 's it all about ? john . chapter nine . verse twenty-five .\n",
      "Target:   you want to know what it is ? what it 's all about ? john . chapter nine . verse twenty-five .\n",
      "\n",
      "Decoding: i only mention it because i took a test this afternoon , down on montgomery street .\n",
      "Target:   i only mention it because i took the test this afternoon , down on montgomery street .\n",
      "\n",
      "Decoding: christine ! mister van orton is valued customer ...\n",
      "Target:   christine ! mister van orton is a valued customer ...\n",
      "\n",
      "Decoding: a single ?\n",
      "Target:   single ?\n",
      "\n",
      "Decoding: there 's another gig starting in saudi arabia . i just a walk-on this time though . bit-part .\n",
      "Target:   there 's another gig starting in saudi arabia . i 'm just a walk-on this time though . bit-part .\n",
      "\n",
      "Decoding: no ! you take another step , i shoot ! they 're trying to kill me ...\n",
      "Target:   no ! you take another step , i 'll shoot ! they 're trying to kill me ...\n",
      "\n",
      "Decoding: listen very carefully , i 'm telling the truth ... this is a game . this was all the game .\n",
      "Target:   listen very carefully , i 'm telling the truth ... this is the game . this was all the game .\n",
      "\n",
      "Decoding: that 's gun . that 's ... that 's not automatic . the guard had an automatic ...\n",
      "Target:   that gun . that ... that 's not automatic . the guard had an automatic ...\n",
      "\n",
      "Decoding: take a picture out .\n",
      "Target:   take the picture out .\n",
      "\n",
      "Decoding: yeah . first communion . are n't i little angel ?\n",
      "Target:   yeah . first communion . are n't i a little angel ?\n",
      "\n",
      "Decoding: let me go get some clothes on . we talk , okay ? be right back .\n",
      "Target:   let me go get some clothes on . we 'll talk , okay ? be right back .\n",
      "\n",
      "Decoding: i 'm tired . i 'm sorry , i should go . i 've been enough of nuisance .\n",
      "Target:   i 'm tired . i 'm sorry , i should go . i 've been enough of a nuisance .\n",
      "\n",
      "Decoding: they said five hundred . i said six . they said man in the gray flannel suit . i think i said , you mean the attractive guy in the gray flannel suit ?\n",
      "Target:   they said five hundred . i said six . they said the man in the gray flannel suit . i think i said , you mean the attractive guy in the gray flannel suit ?\n",
      "\n",
      "Decoding: i have a confession to make . someone gave me six-hundred dollars to spill a drinks on you , as a practical joke .\n",
      "Target:   i have a confession to make . someone gave me six-hundred dollars to spill drinks on you , as a practical joke .\n",
      "\n",
      "Decoding: maitre d ' called you christine .\n",
      "Target:   the maitre d ' called you christine .\n",
      "\n",
      "Decoding: i know owner of campton place . i could talk to him in the morning .\n",
      "Target:   i know the owner of campton place . i could talk to him in the morning .\n",
      "\n",
      "Decoding: fresh shirt ...\n",
      "Target:   a fresh shirt ...\n",
      "\n",
      "Decoding: investment banking . moving money from a place to place .\n",
      "Target:   investment banking . moving money from place to place .\n",
      "\n",
      "Decoding: what 's the c .r .s . ?\n",
      "Target:   what 's c .r .s . ?\n",
      "\n",
      "Decoding: this is a c .r .s .\n",
      "Target:   this is c .r .s .\n",
      "\n",
      "Decoding: their ladder here .\n",
      "Target:   there 's a ladder here .\n",
      "\n",
      "Decoding: this is n't attempt to be gallant . if i do n't lift you , how are you going to get there ?\n",
      "Target:   this is n't an attempt to be gallant . if i do n't lift you , how are you going to get there ?\n",
      "\n",
      "Decoding: are you suggesting we wait till someone 's finds us ?\n",
      "Target:   are you suggesting we wait till someone finds us ?\n",
      "\n",
      "Decoding: `` ... wait for help . '' wait for help . i 'm not opening that specifically warns me not to .\n",
      "Target:   `` ... wait for help . '' wait for help . i 'm not opening a door that specifically warns me not to .\n",
      "\n",
      "Decoding: read what it says : `` warning , do < u > not < /u > attempt to open . if elevator stops , use the emergency ... ``\n",
      "Target:   read what it says : `` warning , do < u > not < /u > attempt to open . if elevator stops , use emergency ... ``\n",
      "\n",
      "Decoding: long story . i found this key in the mouth of wooden harlequin .\n",
      "Target:   long story . i found this key in the mouth of a wooden harlequin .\n",
      "\n",
      "Decoding: how do you know that way ?\n",
      "Target:   how do you know that 's the way ?\n",
      "\n",
      "Decoding: it 's run by company ... they play elaborate pranks . things like this . i 'm really only now finding out myself .\n",
      "Target:   it 's run by a company ... they play elaborate pranks . things like this . i 'm really only now finding out myself .\n",
      "\n",
      "Decoding: you got to be kidding .\n",
      "Target:   you 've got to be kidding .\n",
      "\n",
      "Decoding: i do n't think he breathing .\n",
      "Target:   i do n't think he 's breathing .\n",
      "\n",
      "Decoding: a bad month . you did exact the same thing to me last week .\n",
      "Target:   a bad month . you did the exact same thing to me last week .\n",
      "\n",
      "Decoding: yeah , yeah . she 's called a cab . said something about catching plane .\n",
      "Target:   yeah , yeah . she called a cab . said something about catching a plane .\n",
      "\n",
      "Decoding: oh , god yes please . thanks , man . i take you up on that .\n",
      "Target:   oh , god yes please . thanks , man . i 'll take you up on that .\n",
      "\n",
      "Decoding: this ... ? oh , this is just ... this is bill .\n",
      "Target:   this ... ? oh , this is just ... this is the bill .\n",
      "\n",
      "Decoding: baby , they were all over the house with metal detectors . they switched your gun with look-alike , rigged barrel , loaded with blanks . pop-gun .\n",
      "Target:   baby , they were all over the house with metal detectors . they switched your gun with a look-alike , rigged barrel , loaded with blanks . pop-gun .\n",
      "\n",
      "Decoding: you dodged bullet .\n",
      "Target:   you dodged a bullet .\n",
      "\n",
      "Decoding: c .r .s . who do you think ? jesus h . , thank your lucky charms . to think what i 've almost got you into .\n",
      "Target:   c .r .s . who do you think ? jesus h . , thank your lucky charms . to think what i almost got you into .\n",
      "\n",
      "Decoding: it 's profound life experience .\n",
      "Target:   it 's a profound life experience .\n",
      "\n",
      "Decoding: you 've heard of it . you 've seen other people having it . they 're entertainment service , but more than that .\n",
      "Target:   you 've heard of it . you 've seen other people having it . they 're an entertainment service , but more than that .\n",
      "\n",
      "Decoding: they make your life fun . there 's only guarantee is you will not be bored .\n",
      "Target:   they make your life fun . their only guarantee is you will not be bored .\n",
      "\n",
      "Decoding: not after i done with it . actually , i 've been here . in grad-school i bought crystal-meth from the maitre d ' .\n",
      "Target:   not after i 'm done with it . actually , i 've been here . in grad-school i bought crystal-meth from the maitre d ' .\n",
      "\n",
      "Decoding: that 's why it 's a classic . come on , man ... how 'bout hug ... ?\n",
      "Target:   that 's why it 's a classic . come on , man ... how 'bout a hug ... ?\n",
      "\n",
      "Decoding: how much is it ? a few thousand , at least . a rolex like that ... lucky for you 've missed it .\n",
      "Target:   how much is it ? a few thousand , at least . a rolex like that ... lucky for you they missed it .\n",
      "\n",
      "Decoding: i told you , they hired me over the phone . i 've never met anyone .\n",
      "Target:   i told you , they hired me over the phone . i never met anyone .\n",
      "\n",
      "Decoding: i do n't want money . i 'm pulling back curtain . i 'm here to meet the wizard .\n",
      "Target:   i do n't want money . i 'm pulling back the curtain . i 'm here to meet the wizard .\n",
      "\n",
      "Decoding: tell them the cops are after you ... tell them you got to talk to someone , i 'm threatening to blow the whistle .\n",
      "Target:   tell them the cops are after you ... tell them you 've got to talk to someone , i 'm threatening to blow the whistle .\n",
      "\n",
      "Decoding: they own the whole building . they just move from the floor to floor .\n",
      "Target:   they own the whole building . they just move from floor to floor .\n",
      "\n",
      "Decoding: look , it was just a job . nothing personal , ya know ? i play my part , improvise little . that 's what i 'm good at .\n",
      "Target:   look , it was just a job . nothing personal , ya know ? i play my part , improvise a little . that 's what i 'm good at .\n",
      "\n",
      "Decoding: that 's right -- you 're left-brain the word fetishist .\n",
      "Target:   that 's right -- you 're a left-brain word fetishist .\n",
      "\n",
      "Decoding: one guarantee . payment 's entirely at your brother discretion and , as a gift , dependent on your satisfaction .\n",
      "Target:   one guarantee . payment 's entirely at your brother 's discretion and , as a gift , dependent on your satisfaction .\n",
      "\n",
      "Decoding: your brother was a client with our branch . we do a sort of informal scoring . his numbers were outstanding . sure you 're not hungry at all ... ? tung hoy , best in chinatown ...\n",
      "Target:   your brother was a client with our london branch . we do a sort of informal scoring . his numbers were outstanding . sure you 're not hungry at all ... ? tung hoy , best in chinatown ...\n",
      "\n",
      "Decoding: key ?\n",
      "Target:   the key ?\n",
      "\n",
      "Decoding: nobody 's worried about your father .\n",
      "Target:   nobody worried about your father .\n",
      "\n",
      "Decoding: there 's been a break in . lock this door and stay here . do n't move muscle .\n",
      "Target:   there 's been a break in . lock this door and stay here . do n't move a muscle .\n",
      "\n",
      "Decoding: i do n't know what you 're talking about . what happened ?\n",
      "Target:   i do n't know what you 're talking about . what 's happened ?\n",
      "\n",
      "Decoding: did alarm go off ? the house ... they ... you did n't see ... ?\n",
      "Target:   did the alarm go off ? the house ... they ... you did n't see ... ?\n",
      "\n",
      "Decoding: then then .\n",
      "Target:   goodnight then .\n",
      "\n",
      "Decoding: okay . i think he into some sort of new personal improvement cult .\n",
      "Target:   okay . i think he 's into some sort of new personal improvement cult .\n",
      "\n",
      "Decoding: dinner in the oven .\n",
      "Target:   dinner 's in the oven .\n",
      "\n",
      "Decoding: there was incident a few days ago ... a nervous breakdown , they said . the police took him . they left this address , in case anyone ...\n",
      "Target:   there was an incident a few days ago ... a nervous breakdown , they said . the police took him . they left this address , in case anyone ...\n",
      "\n",
      "Decoding: what 's trouble ?\n",
      "Target:   what 's the trouble ?\n",
      "\n",
      "Decoding: mister ... seymour butts .\n",
      "Target:   a mister ... seymour butts .\n",
      "\n",
      "Decoding: what 's the gentleman , maria ?\n",
      "Target:   what gentleman , maria ?\n",
      "\n",
      "Decoding: i would n't mention following , except he was very insistent . it 's obviously some sort of prank ...\n",
      "Target:   i would n't mention the following , except he was very insistent . it 's obviously some sort of prank ...\n",
      "\n",
      "Decoding: i send your regrets . honestly , why must i even bother ?\n",
      "Target:   i 'll send your regrets . honestly , why must i even bother ?\n",
      "\n",
      "Decoding: the hinchberger 's wedding .\n",
      "Target:   the hinchberger wedding .\n",
      "\n",
      "Decoding: invitations : museum gala .\n",
      "Target:   invitations : the museum gala .\n",
      "\n",
      "Decoding: nice touch . does a game use real bullets ... ?\n",
      "Target:   nice touch . does the game use real bullets ... ?\n",
      "\n",
      "Decoding: it 's what they do . it 's like ... being toyed with by a bunch of ... depraved children\n",
      "Target:   it 's what they do . it 's like ... being toyed with by a bunch of ... depraved children .\n",
      "\n",
      "Decoding: find out about a company called the c .r .s . consumer recreation services .\n",
      "Target:   find out about a company called c .r .s . consumer recreation services .\n",
      "\n",
      "Decoding: someone 's playing hardball . it 's complicated . can i ask favor ?\n",
      "Target:   someone 's playing hardball . it 's complicated . can i ask a favor ?\n",
      "\n",
      "Decoding: how 's the concerned should i be ?\n",
      "Target:   how concerned should i be ?\n",
      "\n",
      "Decoding: that you 've a involved conrad ... is unforgivable . i am now your enemy .\n",
      "Target:   that you 've involved conrad ... is unforgivable . i am now your enemy .\n",
      "\n",
      "Decoding: what happened ...\n",
      "Target:   what 's happened ...\n",
      "\n",
      "Decoding: modelling small-group dynamics in formation of narrative hallucinations . you brought us here to scare us . insomnia , that was just a decoy issue . you 're disgusting .\n",
      "Target:   modelling small-group dynamics in the formation of narrative hallucinations . you brought us here to scare us . insomnia , that was just a decoy issue . you 're disgusting .\n",
      "\n",
      "Decoding: come on . these are the typically sentimental gestures of depraved industrialist .\n",
      "Target:   come on . these are the typically sentimental gestures of a depraved industrialist .\n",
      "\n",
      "Decoding: the children . children hugh crain built the house for . the children he never had .\n",
      "Target:   the children . the children hugh crain built the house for . the children he never had .\n",
      "\n",
      "Decoding: obsessive worrier . join club . and you ? i 'd guess ...\n",
      "Target:   obsessive worrier . join the club . and you ? i 'd guess ...\n",
      "\n",
      "Decoding: so why did you need the addam family mansion for a scientific test ?\n",
      "Target:   so why did you need the addam 's family mansion for a scientific test ?\n",
      "\n",
      "Decoding: -- how much is this car 's worth ?\n",
      "Target:   -- how much is this car worth ?\n",
      "\n",
      "Decoding: you do n't really believe it haunted ... do you believe in ghosts ?\n",
      "Target:   you do n't really believe it 's haunted ... do you believe in ghosts ?\n",
      "\n",
      "Decoding: so could you ! is this some fucked up the idea of art , putting someone else 's name to a painting ?\n",
      "Target:   so could you ! is this some fucked up idea of art , putting someone else 's name to a painting ?\n",
      "\n",
      "Decoding: and why did n't marrow tell < u > us < /u > ? does n't he a trust women ? that fuck .\n",
      "Target:   and why did n't marrow tell < u > us < /u > ? does n't he trust women ? that fuck .\n",
      "\n",
      "Decoding: nah , you 're going crazy with doubt , all of your mistakes are coming back up the pipes , and it 's worse than nightmare . --\n",
      "Target:   nah , you 're going crazy with doubt , all of your mistakes are coming back up the pipes , and it 's worse than a nightmare . --\n",
      "\n",
      "Decoding: not the way you 've constructed your group , it just not ethical !\n",
      "Target:   not the way you 've constructed your group , it 's just not ethical !\n",
      "\n",
      "Decoding: children want me . they 're calling me . they need me .\n",
      "Target:   the children want me . they 're calling me . they need me .\n",
      "\n",
      "Decoding: i looked at theo . she had look on her face .\n",
      "Target:   i looked at theo . she had a look on her face .\n",
      "\n",
      "Decoding: i was n't thinking about my mother bathroom .\n",
      "Target:   i was n't thinking about my mother 's bathroom .\n",
      "\n",
      "Decoding: so ... smell ... is ... smell is sense that triggers the most powerful memories . and memory can trigger a smell .\n",
      "Target:   so ... smell ... is ... smell is the sense that triggers the most powerful memories . and a memory can trigger a smell .\n",
      "\n",
      "Decoding: in the bathroom in my mother 's room , toilet was next to old wooden table . it smelled like that wood .\n",
      "Target:   in the bathroom in my mother 's room , the toilet was next to an old wooden table . it smelled like that wood .\n",
      "\n",
      "Decoding: cold sensation . who felt it first ?\n",
      "Target:   the cold sensation . who felt it first ?\n",
      "\n",
      "Decoding: i really ... honored to be part of this study , jim .\n",
      "Target:   i 'm really ... honored to be part of this study , jim .\n",
      "\n",
      "Decoding: nell . good enough . and i jim .\n",
      "Target:   nell . good enough . and i 'm jim .\n",
      "\n",
      "Decoding: that ? that 's a hill house .\n",
      "Target:   that ? that 's hill house .\n",
      "\n",
      "Decoding: here 's how they 're organized . groups of five , very different personalities : scored all over the kiersey temperament sorter just like you asked for . and they all score high on insomnia charts .\n",
      "Target:   here 's how they 're organized . groups of five , very different personalities : scored all over the kiersey temperament sorter just like you asked for . and they all score high on the insomnia charts .\n",
      "\n",
      "Decoding: you hear the vibrations in the wire . there 's magnetic pulse in the wires , you feel it . i could test it .\n",
      "Target:   you hear the vibrations in the wire . there 's a magnetic pulse in the wires , you feel it . i could test it .\n",
      "\n",
      "Decoding: but experiment was a failure .\n",
      "Target:   but the experiment was a failure .\n",
      "\n",
      "Decoding: he wandering around house , and nell heard him . she thought it was ghosts . let 's go look for him again .\n",
      "Target:   he 's wandering around the house , and nell heard him . she thought it was ghosts . let 's go look for him again .\n",
      "\n",
      "Decoding: i 'll take her with me to university tomorrow . i ca n't believe i read the test wrong . i did n't see anything that looked like she was suicidal .\n",
      "Target:   i 'll take her with me to the university tomorrow . i ca n't believe i read the test wrong . i did n't see anything that looked like she was suicidal .\n",
      "\n",
      "Decoding: no , but nell been here longer than i have .\n",
      "Target:   no , but nell 's been here longer than i have .\n",
      "\n",
      "Decoding: rene crain . up there . rope . ship 's hawser . hard to tie . do n't know how she 's got it .\n",
      "Target:   rene crain . up there . rope . ship 's hawser . hard to tie . do n't know how she got it .\n",
      "\n",
      "Decoding: mrs . dudley be waiting for you .\n",
      "Target:   mrs . dudley 'll be waiting for you .\n",
      "\n",
      "Decoding: that 's a good question . what is it about fences ? sometimes a locked chain makes people on both sides of fence just a little more comfortable . why would that be ?\n",
      "Target:   that 's a good question . what is it about fences ? sometimes a locked chain makes people on both sides of the fence just a little more comfortable . why would that be ?\n",
      "\n",
      "Decoding: well , i 've never lived with a beauty . you must love working here .\n",
      "Target:   well , i 've never lived with beauty . you must love working here .\n",
      "\n",
      "Decoding: nell , it makes sense . it 's all makes sense . you and i , we were scaring each other , working each other up .\n",
      "Target:   nell , it makes sense . it all makes sense . you and i , we were scaring each other , working each other up .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for decoding, target in errors:\n",
    "    print(\"Decoding: \" + \" \".join(decoding))\n",
    "    print(\"Target:   \" + \" \".join(target) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
